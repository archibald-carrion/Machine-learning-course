{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53406c19-9df7-46cc-9fd6-6cfa7ec494a1",
   "metadata": {},
   "source": [
    "# Red Neuronal Perceptrón Multicapa (MLP)para Clasificación de Iris Setosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b904b19-ec26-4920-82f0-f70e4c5d2e72",
   "metadata": {},
   "source": [
    "Vamos a crear una red neuronal MLP pequeña para clasificar la flor Iris Setosa usando sólo NumPy. Implementaremos todo desde cero, incluyendo:\n",
    "\n",
    "* Pasada hacia adelante\n",
    "* Función de pérdida (entropía cruzada)\n",
    "* Retropropagación del error\n",
    "* Actualización de parámetros\n",
    "\n",
    "Ejecute las celdas, pero estudie el código y el texto para comprender el funcionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1391b29-ef6b-48d2-8e94-16910d6c3fe6",
   "metadata": {},
   "source": [
    "## Importación de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a08fd7-f06c-4223-8ac2-e09a53fa98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f2f46-676b-4761-b190-8877057b104b",
   "metadata": {},
   "source": [
    "## Clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b74880-e00a-4fc4-97bb-2b9c00085c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    Una red neuronal perceptrón multicapa (MLP) para clasificación multiclase.\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Capa de entrada: 4 neuronas (características de las flores)\n",
    "    - Capa oculta: 5 neuronas con activación ReLU\n",
    "    - Capa de salida: 3 neuronas con activación softmax (para clasificación de 3 clases)\n",
    "    \n",
    "    Usamos entropía cruzada categórica como función de pérdida.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=4, hidden_size=5, output_size=3):\n",
    "        \"\"\"\n",
    "        Inicializa los parámetros de la red.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Número de características de entrada\n",
    "            hidden_size (int): Número de neuronas en la capa oculta\n",
    "            output_size (int): Número de neuronas en la capa de salida (clases)\n",
    "        \"\"\"\n",
    "        # Inicialización de pesos con Xavier/Glorot initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante (forward pass).\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): Datos de entrada (shape: n_muestras x n_características)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (salida_capa_oculta, salida_red)\n",
    "        \"\"\"\n",
    "        # Capa oculta\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1  # Combinación lineal\n",
    "        self.a1 = self.relu(self.z1)             # Activación ReLU\n",
    "        \n",
    "        # Capa de salida\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Combinación lineal\n",
    "        self.a2 = self.softmax(self.z2)               # Activación softmax\n",
    "        \n",
    "        return self.a1, self.a2\n",
    "    \n",
    "    def backward(self, X, y, a1, a2, learning_rate):\n",
    "        \"\"\"\n",
    "        Propagación hacia atrás (backward pass) y actualización de pesos.\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): Datos de entrada\n",
    "            y (np.array): Etiquetas verdaderas (one-hot encoded)\n",
    "            a1 (np.array): Salida de la capa oculta\n",
    "            a2 (np.array): Salida de la red (predicciones)\n",
    "            learning_rate (float): Tasa de aprendizaje\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Número de muestras\n",
    "        \n",
    "        # Cálculo de gradientes\n",
    "        # Error en la capa de salida (derivada de la entropía cruzada + softmax)\n",
    "        dz2 = a2 - y\n",
    "        \n",
    "        # Gradientes para los parámetros de la capa de salida\n",
    "        dW2 = (1/m) * np.dot(a1.T, dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Error en la capa oculta (propagación hacia atrás a través de ReLU)\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.relu_derivative(a1)\n",
    "        \n",
    "        # Gradientes para los parámetros de la capa oculta\n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Actualización de parámetros (descenso de gradiente)\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcula la pérdida de entropía cruzada categórica.\n",
    "        \n",
    "        Args:\n",
    "            y_true (np.array): Etiquetas verdaderas (one-hot encoded)\n",
    "            y_pred (np.array): Predicciones de la red\n",
    "            \n",
    "        Returns:\n",
    "            float: Valor de la pérdida\n",
    "        \"\"\"\n",
    "        # Evitar log(0) agregando un pequeño valor\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        return loss\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"Función de activación softmax.\"\"\"\n",
    "        # Estabilidad numérica: restar el máximo\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        \"\"\"Función de activación ReLU.\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, a):\n",
    "        \"\"\"Derivada de la función ReLU.\"\"\"\n",
    "        return (a > 0).astype(float)\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01, verbose=100):\n",
    "        \"\"\"\n",
    "        Entrena la red neuronal.\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): Datos de entrenamiento\n",
    "            y (np.array): Etiquetas (one-hot encoded)\n",
    "            epochs (int): Número de épocas de entrenamiento\n",
    "            learning_rate (float): Tasa de aprendizaje\n",
    "            verbose (int): Frecuencia de impresión de información\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Forward pass\n",
    "            a1, a2 = self.forward(X)\n",
    "            \n",
    "            # Cálculo de pérdida\n",
    "            loss = self.compute_loss(y, a2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass y actualización de pesos\n",
    "            self.backward(X, y, a1, a2, learning_rate)\n",
    "            \n",
    "            if verbose and epoch % verbose == 0:\n",
    "                print(f\"Época {epoch}, Pérdida: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Realiza predicciones.\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): Datos de entrada\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Clases predichas (0, 1 o 2)\n",
    "        \"\"\"\n",
    "        _, a2 = self.forward(X)\n",
    "        return np.argmax(a2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50813e9a-1998-415b-aa7f-77ebb48a9420",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f51918-5f26-4be6-a5b9-f304296162cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y preparar datos de Iris para clasificación multiclase\n",
    "def load_iris_data_multiclass():\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target.reshape(-1, 1)  # Convertir a 2D para OneHotEncoder\n",
    "    \n",
    "    # Convertir etiquetas a one-hot encoding\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_onehot = encoder.fit_transform(y)\n",
    "    \n",
    "    # Dividir en conjuntos de entrenamiento y prueba CON STRATIFY\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Normalizar características\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79507bb4-a7aa-4adb-9483-3aaa57e820d6",
   "metadata": {},
   "source": [
    "## Entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3644f-2bde-454b-aa5d-f1c9ff02c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "''' Entrenar y evaluar el modelo multiclase '''\n",
    "\n",
    "# Cargar datos\n",
    "X_train, X_test, y_train, y_test, encoder = load_iris_data_multiclass()\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "model = MLP(input_size=4, hidden_size=5, output_size=3)\n",
    "losses = model.train(X_train, y_train, epochs=5000, learning_rate=0.01, verbose=250)\n",
    "\n",
    "# Evaluar\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "# Convertir one-hot encoding de vuelta a labels\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calcular exactitud usando sklearn.metrics.accuracy_score\n",
    "train_acc = accuracy_score(y_train_labels, train_preds)\n",
    "test_acc = accuracy_score(y_test_labels, test_preds)\n",
    "\n",
    "print(f\"\\nExactitud en entrenamiento: {train_acc:.2%}\")\n",
    "print(f\"Exactitud en prueba: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3302f-610d-4dc2-a483-86df78722a11",
   "metadata": {},
   "source": [
    "## Explicación Detallada\n",
    "\n",
    "### 1. Inicialización\n",
    "\n",
    "Los pesos se inicializan usando la inicialización Xavier/Glorot, que ayuda a que los gradientes fluyan mejor al inicio.\n",
    "\n",
    "`W1` y `b1` son los pesos y sesgos de la capa oculta.\n",
    "\n",
    "`W2` y `b2` son los pesos y sesgos de la capa de salida.\n",
    "\n",
    "### 2. Pasada hacia adelante\n",
    "\n",
    "#### Capa oculta:\n",
    "\n",
    "`z1 = X·W1 + b1`: Combinación lineal de entradas y pesos\n",
    "\n",
    "`a1 = ReLU(z1)`: Aplicación de función de activación ReLU\n",
    "\n",
    "### Capa de salida:\n",
    "\n",
    "`z2 = a1·W2 + b2`: Combinación lineal\n",
    "\n",
    "`a2 = softmax(z2)`: Función Softmax para obtener probabilidades entre 0 y 1\n",
    "\n",
    "```python\n",
    "def softmax(self, z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Para estabilidad numérica\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "```\n",
    "\n",
    "Softmax garantiza que las salidas sumen 1 (como probabilidades)\n",
    "\n",
    "### 3. Cálculo de Pérdida\n",
    "\n",
    "Usamos entropía cruzada binaria categórica:\n",
    "\n",
    "```python\n",
    "loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "`y_true` es _one-hot encoded_ (ej. [1, 0, 0] para clase 0)\n",
    "\n",
    "`y_pred` son las probabilidades de softmax\n",
    "\n",
    "### 4. Retropropagación del error\n",
    "\n",
    "#### Capa de salida:\n",
    "\n",
    "El gradiente para softmax + entropía cruzada es sorprendentemente simple:\n",
    "\n",
    "Error:\n",
    "```python\n",
    "dz2 = a2 - y  # derivada de la pérdida respecto a z2, igual que en el caso binario\n",
    "```\n",
    "\n",
    "#### Gradientes:\n",
    "\n",
    "`dW2 = (1/m) * a1.T · dz2`\n",
    "\n",
    "`db2 = (1/m) * sum(dz2)`\n",
    "\n",
    "#### Capa oculta:\n",
    "\n",
    "Error: `dz1 = (dz2 · W2.T) * ReLU'(a1)` (regla de la cadena)\n",
    "\n",
    "#### Gradientes:\n",
    "\n",
    "`dW1 = (1/m) * X.T · dz1`\n",
    "\n",
    "`db1 = (1/m) * sum(dz1)`\n",
    "\n",
    "### 5. Funciones de Activación\n",
    "\n",
    "**ReLU**: `max(0, x)` - Simple y evita el problema de gradientes vanishing\n",
    "\n",
    "**Sigmoid**: `1/(1 + e^-x)` - Para obtener probabilidades en la salida\n",
    "\n",
    "### 6. Preparación de Datos\n",
    "\n",
    "_One-hot encoding_ de las etiquetas:\n",
    "\n",
    "```python\n",
    "# Antes: [0, 1, 2]\n",
    "# Después: [[1,0,0], [0,1,0], [0,0,1]]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "```\n",
    "\n",
    "### 7. Predicción\n",
    "\n",
    "Para obtener la clase predicha:\n",
    "\n",
    "```python\n",
    "np.argmax(a2, axis=1)  # Devuelve el índice de la neurona con mayor probabilidad\n",
    "```\n",
    "\n",
    "### 8. Entrenamiento\n",
    "\n",
    "El bucle de entrenamiento:\n",
    "\n",
    "1. Calcula la pasada hacia adelante\n",
    "2. Calcula la pérdida\n",
    "3. Realiza la pasada hacia atrás\n",
    "4. Actualiza pesos con descenso de gradiente\n",
    "\n",
    "### 9. Explicación Matemática Detallada\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "Para un vector de logits z = [z₁, z₂, z₃]:\n",
    "\n",
    "$p_i = e^{z_i} / (∑_{j=1}^3 e^{z_j})$\n",
    "\n",
    "#### Entropía Cruzada Categórica\n",
    "\n",
    "Para una muestra con etiqueta verdadera y (one-hot) y predicción ŷ:\n",
    "\n",
    "$L = -∑_{i=1}^3 y_i log(ŷ_i)$\n",
    "\n",
    "#### Gradientes\n",
    "\n",
    "La derivada de la pérdida respecto a los logits de salida (z₂) es:\n",
    "\n",
    "$∂L/∂z₂ = ŷ - y$\n",
    "\n",
    "Este gradiente es idéntico al caso binario, pero opera sobre vectores en lugar de escalares.\n",
    "\n",
    "### 10. Resultados Esperados\n",
    "\n",
    "Con este modelo se debería obtener:\n",
    "\n",
    "* Precisión en entrenamiento > 95%\n",
    "* Precisión en prueba > 90%\n",
    "\n",
    "La red aprenderá a distinguir las 3 clases de flores Iris.\n",
    "\n",
    "Este código implementa todos los componentes fundamentales de una red neuronal desde cero, usando solo operaciones matriciales de NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592ca44-0522-4d1a-8aae-ed710496a352",
   "metadata": {},
   "source": [
    "# Red Neuronal MLP para Iris con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee98bc-5a31-42e5-afae-2f4f615298c3",
   "metadata": {},
   "source": [
    "Esta es una implementación completa en PyTorch para clasificar las flores Iris (3 clases), con:\n",
    "\n",
    "* 5000 épocas de entrenamiento\n",
    "* Matriz de confusión\n",
    "* Reporte de métricas (precision, recall, f1-score)\n",
    "* Uso de funciones de accuracy de PyTorch/scikit-learn\n",
    "* Código bien comentado comparando con la versión manual anterior\n",
    "\n",
    "Al igual que con la implementación anterior corra la celda pero estudie el código con sus comentarios para comprender el funcionamiento y aprender a utilizar Pytorch.\n",
    "\n",
    "Nota: Si desean utilizar GPU en lugar de CPU deben hacer una ligera modificación al código, aprenda como hacerlo investigando al respecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba43beb-a493-4088-88f4-e733b0065a0d",
   "metadata": {},
   "source": [
    "## Importación de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64148fc-933d-4b46-a7bb-8add6157fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac7719-935e-4c7c-8e0f-c4052438ee7e",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa9f02-e01e-4047-b60b-f6c10c23ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar y preparar datos (similar a load_iris_data anterior)\n",
    "def load_iris_data():\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    # Dividir en train/test (80/20) estratificado\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalizar (como en la versión manual)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Convertir a tensores PyTorch\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.LongTensor(y_train)  # Para CrossEntropyLoss\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13011f04-95f8-49ca-8cbc-ec0946a88f63",
   "metadata": {},
   "source": [
    "## Clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b14b03-c3ca-451a-87e3-36fa7307fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Definir la red neuronal (análogo a MLP anterior)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=5, output_size=3):\n",
    "        super().__init__()\n",
    "        # Capa oculta (como W1 y b1 en la versión manual)\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        # Capa de salida (como W2 y b2)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        # Función de activación ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass (igual que en la versión manual)\n",
    "        x = self.hidden(x)      # z1 = X·W1 + b1\n",
    "        x = self.relu(x)        # a1 = ReLU(z1)\n",
    "        x = self.output(x)      # z2 = a1·W2 + b2\n",
    "        # No aplicamos softmax aquí, CrossEntropyLoss lo incluye\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586825f0-7a48-4e69-b88d-f114d36e9168",
   "metadata": {},
   "source": [
    "## Entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64221de-c23d-402b-9162-a39a9bfef925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "# 3. Entrenamiento (análogo al train anterior pero con PyTorch)\n",
    "def train_model(model, X_train, y_train, epochs=5000, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()  # Incluye softmax + entropía cruzada\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # Optimizador más avanzado\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # PPasada hacia adelante\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Pasada hacia atrás y optimización (retropropagación automática)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Época {epoch}, Pérdida: {loss.item():.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 4. Evaluación y métricas\n",
    "def plot_confusion_matrix(cm, classes, title='Matriz de Confusión'):\n",
    "    \"\"\"Matriz combinada con valores absolutos y relativos\"\"\"\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "    plt.title(title, pad=20, fontsize=14)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "    \n",
    "    # Umbral para color del texto\n",
    "    thresh = 0.5\n",
    "    \n",
    "    # Añadir valores absolutos y porcentajes\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i-0.15, f\"{cm[i, j]}\",  # Valor absoluto arriba\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm_norm[i, j] > thresh else 'black',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.text(j, i+0.15, f\"({cm_norm[i, j]:.1%})\",  # Porcentaje abajo\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm_norm[i, j] > thresh else 'black',\n",
    "                 fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta Verdadera', fontsize=12)\n",
    "    plt.xlabel('Predicción', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, target_names):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        print(f'\\nExactitud: {acc:.2%}')\n",
    "        \n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        print('\\nMatriz de Confusión:')\n",
    "        print(cm)\n",
    "        \n",
    "        print('\\nReporte de Clasificación:')\n",
    "        print(classification_report(y_test, preds, target_names=target_names))\n",
    "        \n",
    "        plot_confusion_matrix(cm, target_names)\n",
    "\n",
    "# 5. Ejecución principal\n",
    "# Cargar datos\n",
    "X_train, X_test, y_train, y_test, target_names = load_iris_data()\n",
    "\n",
    "# Crear modelo\n",
    "model = MLP(input_size=4, hidden_size=5, output_size=3)\n",
    "\n",
    "# Entrenar\n",
    "print(\"Entrenando modelo...\")\n",
    "losses = train_model(model, X_train, y_train, epochs=5000)\n",
    "\n",
    "# Evaluar\n",
    "print(\"\\nEvaluación del modelo:\")\n",
    "evaluate_model(model, X_test, y_test, target_names)\n",
    "\n",
    "# Gráfico de pérdida\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d660df0f-91fd-47bd-9734-5a3c49a25106",
   "metadata": {},
   "source": [
    "# Ejercicio: Implemente un MLP para los datos de diabetes manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a0120-2d3f-4b65-a6fe-cc02d351b941",
   "metadata": {},
   "source": [
    "Implemente un MLP para los datos de indios de diabetes usando el método manual Numpy.  La implementación debe ser manual, igual al primer ejemplo de Iris Setosa.\n",
    "\n",
    "Tenga en cuenta que como es un problema binario, no multiclase debe utilizar como función de activación final la sigmoide, no la softmax.\n",
    "\n",
    "Además, la pérdida debe ser con entropía cruzada binaria. Esta función se define así:\n",
    "\n",
    "$L = -[y·log(ŷ) + (1-y)·log(1-ŷ)]$\n",
    "\n",
    "Donde:\n",
    "\n",
    "`y` es la etiqueta verdadera (0 o 1)\n",
    "\n",
    "`ŷ` es la predicción de la red (probabilidad entre 0 y 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dddf8d-ac0d-42d7-a1b6-b6848021886d",
   "metadata": {},
   "source": [
    "# Ejercicio: Implemente un MLP para los datos de diabetes con Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd3109-3f7e-4a4f-a9f2-808ac1656b2e",
   "metadata": {},
   "source": [
    "Implemente un MLP para los datos de indios de diabetes utilizando Pytorch.  De nuevo, tenga en cuenta que es un problema binario, no multiclase y haga los cambios necesarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd04a4-54f7-4634-a1f0-f733268dba4d",
   "metadata": {},
   "source": [
    "# Ejercicio: Comparación de Regresión Lineal Múltiple, Árbol de Decisión y Perceptrón Multicapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794fb44-b757-4bdf-b0b6-defb11160dee",
   "metadata": {},
   "source": [
    "Copie acá para sus resultados para los datos indios de diabetes los códigos de sus mejores modelos de Regresión Lineal Múltiple y Árbol de Decisión.  Realice una tabla donde compara dichos datos indios de diabetes con las métricas resultado de la Regresión Lineal Múltiple, el Árbol de Decisión y el Perceptrón Multicapa.  Escoja para cada uno la mejor métrica que obtuvo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
