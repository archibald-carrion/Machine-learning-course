{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b0a338-d1c6-4fc3-b8ed-0377ac39a285",
   "metadata": {},
   "source": [
    "# Red Neuronal Convolucional para Clasificación de MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d0365-3001-47ee-bdcf-063c181a7cf0",
   "metadata": {},
   "source": [
    "Vamos a crear una red neuronal convolucional pequeña (aunque más compleja que un perceptrón multicapa) para clasificar el conjunto de datos MNIST (reconocimiento de caracteres numéricos) usando sólo NumPy. Implementaremos todo desde cero, incluyendo:\n",
    "\n",
    "* Pasada hacia adelante\n",
    "* Función de pérdida (entropía cruzada)\n",
    "* Retropropagación del error\n",
    "* Actualización de parámetros\n",
    "\n",
    "Ejecute la celda, pero estudie el código y el texto para comprender el funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30a960-ce5f-445f-8a39-10eda757396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 0. Función para la matriz de confusión\n",
    "def plot_combined_confusion_matrix(cm, classes, title='Matriz de Confusión'):\n",
    "    \"\"\"Matriz combinada con valores absolutos y relativos\"\"\"\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "    plt.title(title, pad=20, fontsize=14)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "    \n",
    "    # Umbral para color del texto\n",
    "    thresh = 0.5\n",
    "    \n",
    "    # Añadir valores absolutos y porcentajes\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i-0.15, f\"{cm[i, j]}\",  # Valor absoluto arriba\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm_norm[i, j] > thresh else 'black',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.text(j, i+0.15, f\"({cm_norm[i, j]:.1%})\",  # Porcentaje abajo\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm_norm[i, j] > thresh else 'black',\n",
    "                 fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta Verdadera', fontsize=12)\n",
    "    plt.xlabel('Predicción', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# 1. Cargar y preparar datos de MNIST\n",
    "def load_mnist_data(test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Carga MNIST y divide en train/val/test\n",
    "    Args:\n",
    "        test_size: Proporción para test\n",
    "        val_size: Proporción de validación (del train)\n",
    "        random_state: Semilla para reproducibilidad\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    print(\"Cargando MNIST...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    # Normalizar y redimensionar para CNN (28x28x1)\n",
    "    X = X.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "    \n",
    "    # Dividir en train+val y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    \n",
    "    # Dividir train en train y val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, stratify=y_train, random_state=random_state)\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# 2. Capas de la CNN\n",
    "class Conv2D:\n",
    "    \"\"\"Capa convolucional 2D\"\"\"\n",
    "    def __init__(self, filters, kernel_size, input_shape, padding='same', activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filters: Número de filtros\n",
    "            kernel_size: Tamaño del kernel (e.g., 3)\n",
    "            input_shape: Forma del input (height, width, channels)\n",
    "            padding: 'same' o 'valid'\n",
    "            activation: 'relu' o None\n",
    "        \"\"\"\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Inicializar pesos (He initialization)\n",
    "        fan_in = kernel_size * kernel_size * input_shape[-1]\n",
    "        self.W = np.random.randn(kernel_size, kernel_size, input_shape[-1], filters) * np.sqrt(2./fan_in)\n",
    "        self.b = np.zeros(filters)\n",
    "        \n",
    "        # Calcular output shape\n",
    "        if padding == 'same':\n",
    "            self.output_shape = (input_shape[0], input_shape[1], filters)\n",
    "        else:\n",
    "            self.output_shape = (input_shape[0]-kernel_size+1, input_shape[1]-kernel_size+1, filters)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        self.X = X\n",
    "        batch_size, h, w, _ = X.shape\n",
    "        \n",
    "        # Aplicar padding si es necesario\n",
    "        if self.padding == 'same':\n",
    "            pad = self.kernel_size // 2\n",
    "            X_padded = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant')\n",
    "        else:\n",
    "            X_padded = X\n",
    "        \n",
    "        # Inicializar output\n",
    "        output = np.zeros((batch_size, *self.output_shape))\n",
    "        \n",
    "        # Convolución\n",
    "        for i in range(self.output_shape[0]):\n",
    "            for j in range(self.output_shape[1]):\n",
    "                receptive_field = X_padded[:, i:i+self.kernel_size, j:j+self.kernel_size, :]\n",
    "                output[:, i, j, :] = np.tensordot(receptive_field, self.W, axes=([1,2,3], [0,1,2])) + self.b\n",
    "        \n",
    "        # Activación\n",
    "        if self.activation == 'relu':\n",
    "            output = np.maximum(0, output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dZ, lr):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        batch_size, h, w, _ = dZ.shape\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        dX = np.zeros_like(self.X)\n",
    "        \n",
    "        # Aplicar padding si es necesario\n",
    "        if self.padding == 'same':\n",
    "            pad = self.kernel_size // 2\n",
    "            X_padded = np.pad(self.X, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant')\n",
    "            dX_padded = np.zeros_like(X_padded)\n",
    "        else:\n",
    "            X_padded = self.X\n",
    "            dX_padded = dX\n",
    "        \n",
    "        # Gradientes\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                receptive_field = X_padded[:, i:i+self.kernel_size, j:j+self.kernel_size, :]\n",
    "                \n",
    "                # Gradiente de los pesos\n",
    "                dW += np.tensordot(receptive_field, dZ[:, i, j, :], axes=([0], [0]))\n",
    "                \n",
    "                # Gradiente del input (si es necesario)\n",
    "                if self.padding != 'valid' or (i < dX.shape[1] and j < dX.shape[2]):\n",
    "                    dX_padded[:, i:i+self.kernel_size, j:j+self.kernel_size, :] += np.tensordot(\n",
    "                        dZ[:, i, j, :], self.W, axes=([1], [3]))\n",
    "        \n",
    "        # Gradiente de los biases\n",
    "        db = np.sum(dZ, axis=(0,1,2))\n",
    "        \n",
    "        # Quitar padding si es necesario\n",
    "        if self.padding == 'same':\n",
    "            pad = self.kernel_size // 2\n",
    "            dX = dX_padded[:, pad:-pad, pad:-pad, :] if pad > 0 else dX_padded\n",
    "        else:\n",
    "            dX = dX_padded\n",
    "        \n",
    "        # Actualizar pesos\n",
    "        self.W -= lr * dW / batch_size\n",
    "        self.b -= lr * db / batch_size\n",
    "        \n",
    "        return dX\n",
    "\n",
    "class MaxPool2D:\n",
    "    \"\"\"Capa de Max Pooling 2D\"\"\"\n",
    "    def __init__(self, pool_size=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        self.X = X\n",
    "        batch_size, h, w, channels = X.shape\n",
    "        self.output_shape = (batch_size, h//self.pool_size, w//self.pool_size, channels)\n",
    "        output = np.zeros(self.output_shape)\n",
    "        self.mask = np.zeros_like(X)\n",
    "        \n",
    "        for i in range(0, h, self.pool_size):\n",
    "            for j in range(0, w, self.pool_size):\n",
    "                receptive_field = X[:, i:i+self.pool_size, j:j+self.pool_size, :]\n",
    "                output[:, i//self.pool_size, j//self.pool_size, :] = np.max(receptive_field, axis=(1,2))\n",
    "                \n",
    "                # Guardar máscara para backprop\n",
    "                max_vals = np.max(receptive_field, axis=(1,2), keepdims=True)\n",
    "                self.mask[:, i:i+self.pool_size, j:j+self.pool_size, :] = (receptive_field == max_vals)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        dX = np.zeros_like(self.X)\n",
    "        batch_size, h_out, w_out, channels = dZ.shape\n",
    "        \n",
    "        for i in range(h_out):\n",
    "            for j in range(w_out):\n",
    "                h_start = i * self.pool_size\n",
    "                w_start = j * self.pool_size\n",
    "                \n",
    "                # Propagamos el gradiente solo a los máximos\n",
    "                dX[:, h_start:h_start+self.pool_size, w_start:w_start+self.pool_size, :] = \\\n",
    "                    self.mask[:, h_start:h_start+self.pool_size, w_start:w_start+self.pool_size, :] * \\\n",
    "                    dZ[:, i:i+1, j:j+1, :]\n",
    "        \n",
    "        return dX\n",
    "\n",
    "class Flatten:\n",
    "    \"\"\"Capa Flatten\"\"\"\n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.reshape(self.input_shape)\n",
    "\n",
    "class Dense:\n",
    "    \"\"\"Capa Fully Connected\"\"\"\n",
    "    def __init__(self, units, input_dim, activation='relu'):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        # Inicialización He\n",
    "        self.W = np.random.randn(input_dim, units) * np.sqrt(2./input_dim)\n",
    "        self.b = np.zeros(units)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.b\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, self.Z)\n",
    "        elif self.activation == 'softmax':\n",
    "            exp = np.exp(self.Z - np.max(self.Z, axis=1, keepdims=True))\n",
    "            return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dA, lr):\n",
    "        batch_size = self.X.shape[0]\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            dZ = dA * (self.Z > 0)\n",
    "        else:\n",
    "            dZ = dA\n",
    "        \n",
    "        dW = np.dot(self.X.T, dZ) / batch_size\n",
    "        db = np.sum(dZ, axis=0) / batch_size\n",
    "        dX = np.dot(dZ, self.W.T)\n",
    "        \n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        return dX\n",
    "\n",
    "# 3. Modelo CNN\n",
    "class CNN:\n",
    "    def __init__(self):\n",
    "        # Arquitectura: Conv -> Pool -> Conv -> Pool -> Flatten -> Dense -> Output\n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=3, input_shape=(28,28,1))\n",
    "        self.pool1 = MaxPool2D(pool_size=2)\n",
    "        self.conv2 = Conv2D(filters=64, kernel_size=3, input_shape=(14,14,32))\n",
    "        self.pool2 = MaxPool2D(pool_size=2)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(units=128, input_dim=7*7*64)\n",
    "        self.output = Dense(units=10, input_dim=128, activation='softmax')\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.conv1.forward(X)\n",
    "        X = self.pool1.forward(X)\n",
    "        X = self.conv2.forward(X)\n",
    "        X = self.pool2.forward(X)\n",
    "        X = self.flatten.forward(X)\n",
    "        X = self.dense.forward(X)\n",
    "        return self.output.forward(X)\n",
    "    \n",
    "    def backward(self, dZ, lr):\n",
    "        dZ = self.output.backward(dZ, lr)\n",
    "        dZ = self.dense.backward(dZ, lr)\n",
    "        dZ = self.flatten.backward(dZ)\n",
    "        dZ = self.pool2.backward(dZ)\n",
    "        dZ = self.conv2.backward(dZ, lr)\n",
    "        dZ = self.pool1.backward(dZ)\n",
    "        dZ = self.conv1.backward(dZ, lr)\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, lr=0.001):\n",
    "        \"\"\"Entrenamiento con tqdm\"\"\"\n",
    "        # One-hot encoding\n",
    "        y_train_oh = np.eye(10)[y_train]\n",
    "        y_val_oh = np.eye(10)[y_val]\n",
    "        \n",
    "        history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "        \n",
    "        # Configurar tqdm para barras anidadas\n",
    "        epoch_pbar = tqdm(range(epochs), desc='Entrenando', unit='epoch', position=0)\n",
    "\n",
    "        for epoch in epoch_pbar:\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            \n",
    "            # Calcular número de batches para la barra interna\n",
    "            num_batches = (len(X_train) + batch_size - 1) // batch_size\n",
    "            \n",
    "            # Barra de progreso para mini-batches\n",
    "            batch_pbar = tqdm(\n",
    "                range(0, len(X_train), batch_size), \n",
    "                desc=f'Época {epoch+1}/{epochs}', \n",
    "                unit='batch',\n",
    "                position=1,\n",
    "                leave=False,\n",
    "                total=num_batches\n",
    "            )\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in batch_pbar:\n",
    "                X_batch = X_train[i:i+batch_size]\n",
    "                y_batch = y_train_oh[i:i+batch_size]\n",
    "                \n",
    "                # Forward\n",
    "                y_pred = self.forward(X_batch)\n",
    "                loss = self.compute_loss(y_batch, y_pred)\n",
    "                \n",
    "                # Backward\n",
    "                dZ = (y_pred - y_batch) / batch_size\n",
    "                self.backward(dZ, lr)\n",
    "                \n",
    "                # Métricas\n",
    "                epoch_loss += loss * len(X_batch)\n",
    "                epoch_acc += accuracy_score(\n",
    "                    np.argmax(y_batch, axis=1),\n",
    "                    np.argmax(y_pred, axis=1)\n",
    "                ) * len(X_batch)\n",
    "                \n",
    "                # Actualizar descripción de la barra de batch con métricas actuales\n",
    "                batch_pbar.set_postfix({\n",
    "                    'loss': f'{loss:.4f}',\n",
    "                    'acc': f'{accuracy_score(np.argmax(y_batch, axis=1), np.argmax(y_pred, axis=1)):.4f}'\n",
    "                })\n",
    "            \n",
    "            batch_pbar.close()\n",
    "            \n",
    "            # Validación\n",
    "            val_pred = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val_oh, val_pred)\n",
    "            val_acc = accuracy_score(y_val, np.argmax(val_pred, axis=1))\n",
    "            \n",
    "            # Guardar historial\n",
    "            avg_loss = epoch_loss / len(X_train)\n",
    "            avg_acc = epoch_acc / len(X_train)\n",
    "            \n",
    "            history['loss'].append(avg_loss)\n",
    "            history['acc'].append(avg_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # Actualizar descripción de la barra de época con todas las métricas\n",
    "            epoch_pbar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'acc': f'{avg_acc:.4f}',\n",
    "                'val_loss': f'{val_loss:.4f}',\n",
    "                'val_acc': f'{val_acc:.4f}'\n",
    "            })\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluación del modelo\"\"\"\n",
    "        y_pred = np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "        print(\"\\nExactitud:\", accuracy_score(y, y_pred))\n",
    "        print(\"\\nReporte de Clasificación:\")\n",
    "        print(classification_report(y, y_pred))\n",
    "        \n",
    "        # Matriz de confusión mejorada\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        plot_combined_confusion_matrix(cm, classes=['0','1','2','3','4','5','6','7','8','9'])\n",
    "\n",
    "# 4. Entrenamiento y evaluación\n",
    "def main():\n",
    "    # Cargar datos\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_mnist_data()\n",
    "    \n",
    "    # Crear y entrenar modelo\n",
    "    model = CNN()\n",
    "    history = model.train(X_train, y_train, X_val, y_val, epochs=35, batch_size=64, lr=0.001)\n",
    "    \n",
    "    # Gráficas de entrenamiento\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Pérdida por Época')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Exactitud por Época')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Exactitud')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluar en test\n",
    "    print(\"\\nEvaluación en Test:\")\n",
    "    model.evaluate(X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb054696-1076-4390-885d-310d9f357eac",
   "metadata": {},
   "source": [
    "## Versión con apoyo para GPU\n",
    "\n",
    "Esta versión es prácticamente igual a la anterior pero con apoyo para GPU lo que permite ahorrar tiempo.\n",
    "\n",
    "### Requisitos para GPU:\n",
    "\n",
    "Para usar GPU necesita instalar CuPy:\n",
    "\n",
    "```bash\n",
    "# Para CUDA 11.x\n",
    "pip install cupy-cuda11x\n",
    "\n",
    "# Para CUDA 12.x  \n",
    "pip install cupy-cuda12x\n",
    "```\n",
    "\n",
    "### Mejoras de rendimiento esperadas:\n",
    "\n",
    "* **CPU**: Sin cambios en velocidad, pero código más limpio.\n",
    "* **GPU: 5-20x más rápido** dependiendo del GPU.\n",
    "* Las operaciones matriciales y convoluciones se aceleran significativamente.\n",
    "\n",
    "El código detecta automáticamente la disponibilidad de GPU e informa qué dispositivo está usando. ¡Esto debería resolver el problema de velocidad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b913f8-bbee-4a49-b0ec-3d4584bdc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Detectar dispositivo y cargar xp = cupy o numpy\n",
    "try:\n",
    "    import cupy as cp\n",
    "    _has_cupy = True\n",
    "except ImportError:\n",
    "    cp = None\n",
    "    _has_cupy = False\n",
    "\n",
    "import numpy as np  # Para fallback cuando no haya CuPy\n",
    "from numpy.lib.stride_tricks import sliding_window_view as np_sliding_window_view\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_xp(device):\n",
    "    \"\"\"\n",
    "    Devuelve el módulo xp: cp si device='gpu' y CuPy está disponible,\n",
    "    en caso contrario NumPy.\n",
    "    \"\"\"\n",
    "    device = device.lower()\n",
    "    if device == 'gpu' and _has_cupy:\n",
    "        return cp\n",
    "    else:\n",
    "        return np\n",
    "\n",
    "# 1. Función para graficar matriz de confusión (convierte a NumPy si es CuPy)\n",
    "def plot_combined_confusion_matrix(cm, classes, title='Matriz de Confusión'):\n",
    "    \"\"\"Matriz combinada con valores absolutos y relativos\"\"\"\n",
    "    if _has_cupy and isinstance(cm, cp.ndarray):\n",
    "        cm_np = cp.asnumpy(cm)\n",
    "    else:\n",
    "        cm_np = cm\n",
    "\n",
    "    cm_norm = cm_np.astype('float') / cm_np.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "    plt.title(title, pad=20, fontsize=14)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "\n",
    "    thresh = 0.5\n",
    "    for i, j in np.ndindex(cm_np.shape):\n",
    "        plt.text(j, i-0.15, f\"{cm_np[i, j]}\",\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm_norm[i, j] > thresh else 'black',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.text(j, i+0.15, f\"({cm_norm[i, j]:.1%})\",\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm_norm[i, j] > thresh else 'black',\n",
    "                 fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta Verdadera', fontsize=12)\n",
    "    plt.xlabel('Predicción', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# 2. Cargar y preparar datos de MNIST (siempre en CPU, porque sklearn retorna NumPy)\n",
    "def load_mnist_data(test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Carga MNIST y divide en train/val/test usando NumPy.\n",
    "    \"\"\"\n",
    "    print(\"Cargando MNIST...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "    # Normalizar y redimensionar\n",
    "    X = X.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "    # Dividir en train+val y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    # Dividir train en train y val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, stratify=y_train, random_state=random_state)\n",
    "\n",
    "    print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# 3. Clases de capas usando im2col / sliding_window_view con xp (NumPy o CuPy)\n",
    "\n",
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    Capa convolucional 2D usando im2col + matmul (BLAS/cuBLAS) y sliding_window_view.\n",
    "    xp: numpy o cupy\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, kernel_size, input_shape, xp, stride=1, padding=1, activation='relu'):\n",
    "        \"\"\"\n",
    "        filters: número de filtros (F)\n",
    "        kernel_size: entero (KH = KW)\n",
    "        input_shape: tupla (H, W, C) de la entrada esperada\n",
    "        xp: módulo (np o cp)\n",
    "        stride: entero (por defecto 1)\n",
    "        padding: entero (por defecto 1 para 'same')\n",
    "        activation: 'relu' o None\n",
    "        \"\"\"\n",
    "        self.filters = filters\n",
    "        self.KH = kernel_size\n",
    "        self.KW = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.xp = xp\n",
    "\n",
    "        # Detectamos y guardamos el número de canales a partir de input_shape\n",
    "        H_in, W_in, C_in = input_shape\n",
    "        self.input_shape = (None, H_in, W_in, C_in)\n",
    "\n",
    "        # Inicialización He: tomamos C directamente\n",
    "        fan_in = kernel_size * kernel_size * C_in\n",
    "        rng = xp.random if xp is not np else np.random\n",
    "        self.W = rng.randn(self.KH, self.KW, C_in, filters, dtype=xp.float32) * xp.sqrt(2.0 / fan_in)\n",
    "        self.b = xp.zeros(filters, dtype=xp.float32)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass vectorizado con im2col / sliding_window_view.\n",
    "        X: (N, H, W, C) — numpy.ndarray o cupy.ndarray\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        N, H, W_in, C_in = X.shape\n",
    "\n",
    "        # 1) Padding si es necesario\n",
    "        if self.padding > 0:\n",
    "            X_padded = xp.pad(\n",
    "                X,\n",
    "                ((0, 0),\n",
    "                 (self.padding, self.padding),\n",
    "                 (self.padding, self.padding),\n",
    "                 (0, 0)),\n",
    "                mode='constant',\n",
    "                constant_values=0\n",
    "            )\n",
    "        else:\n",
    "            X_padded = X\n",
    "\n",
    "        # 2) Calcular dimensiones de salida\n",
    "        H_padded = H + 2*self.padding\n",
    "        W_padded = W_in + 2*self.padding\n",
    "        OH = (H_padded - self.KH) // self.stride + 1\n",
    "        OW = (W_padded - self.KW) // self.stride + 1\n",
    "        self.OH, self.OW = OH, OW\n",
    "\n",
    "        # 3) Extraer parches con sliding_window_view sobre ejes (1,2)\n",
    "        #    NumPy/CuPy devuelven shape = (N, H_padded-KH+1, W_padded-KW+1, C_in, KH, KW)\n",
    "        if xp is np:\n",
    "            patches = np_sliding_window_view(X_padded, (self.KH, self.KW), axis=(1, 2))\n",
    "        else:\n",
    "            patches = xp.lib.stride_tricks.sliding_window_view(\n",
    "                X_padded, (self.KH, self.KW), axis=(1, 2)\n",
    "            )\n",
    "        # Tras la llamada anterior, patches.shape = (N, OH, OW, C_in, KH, KW)\n",
    "        # Hacemos un transpose para pasar a la forma (N, OH, OW, KH, KW, C_in):\n",
    "        patches = patches.transpose(0, 1, 2, 4, 5, 3)  # → (N, OH, OW, KH, KW, C_in)\n",
    "\n",
    "        # Ahora aplicamos el stride:\n",
    "        patches = patches[:, ::self.stride, ::self.stride, :, :, :]  # → (N, OH, OW, KH, KW, C_in)\n",
    "\n",
    "        # Guardamos parches para backward\n",
    "        self.X = X\n",
    "        self.X_padded = X_padded\n",
    "        self.X_patches = patches  # (N, OH, OW, KH, KW, C_in)\n",
    "\n",
    "        # 4) Aplanar parches para matmul: (N*OH*OW, KH*KW*C_in)\n",
    "        Np, OH, OW, KH, KW, Cp = patches.shape\n",
    "        # Cp == C_in\n",
    "        X_col = patches.reshape(Np, OH, OW, KH*KW*Cp)      # (N, OH, OW, KH*KW*C_in)\n",
    "        X_col = X_col.transpose(0, 1, 2, 3).reshape(N*OH*OW, KH*KW*Cp)\n",
    "\n",
    "        # 5) Aplanar pesos W: (KH, KW, C_in, F) → (KH*KW*C_in, F)\n",
    "        C_from_W = self.W.shape[2]  # Éste sí coincide con Cp\n",
    "        W_col = self.W.reshape(self.KH * self.KW * C_from_W, self.filters)\n",
    "\n",
    "        # 6) Matmul grande + bias  → out_col: (N*OH*OW, F)\n",
    "        #    NumPy usará BLAS; CuPy usará cuBLAS/cuDNN.\n",
    "        out_col = X_col.dot(W_col) + self.b.reshape(1, self.filters)\n",
    "\n",
    "        # 7) Reconstruir salida: (N, OH, OW, F)\n",
    "        out = out_col.reshape(N, OH, OW, self.filters)\n",
    "\n",
    "        # 8) Aplicar activación\n",
    "        if self.activation == 'relu':\n",
    "            self.out = xp.maximum(0, out)\n",
    "            return self.out\n",
    "        else:\n",
    "            self.out = out\n",
    "            return out\n",
    "\n",
    "    def backward(self, dZ, lr):\n",
    "        \"\"\"\n",
    "        Retropropagación usando operaciones vectorizadas + scatter.\n",
    "        dZ: (N, OH, OW, F)\n",
    "        lr: learning rate (float)\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        N, H, W_in, C_in = self.X.shape\n",
    "        KH, KW = self.KH, self.KW\n",
    "        stride, padding = self.stride, self.padding\n",
    "        OH, OW = self.OH, self.OW\n",
    "\n",
    "        # 1) Si hay ReLU, enmascarar gradiente\n",
    "        if self.activation == 'relu':\n",
    "            dZ = dZ * (self.out > 0)\n",
    "\n",
    "        # 2) Convertir dZ a dZ_col: (N*OH*OW, F)\n",
    "        dZ_col = dZ.reshape(N * OH * OW, self.filters)\n",
    "\n",
    "        # 3) Reconstruir X_col: (N*OH*OW, KH*KW*C_in) usando los parches guardados\n",
    "        patches = self.X_patches  # (N, OH, OW, KH, KW, C_in)\n",
    "        _, _, _, KH2, KW2, Cp2 = patches.shape  # Cp2 == C_in\n",
    "        X_col = patches.reshape(N, OH, OW, KH2 * KW2 * Cp2).transpose(0, 1, 2, 3)\n",
    "        X_col = X_col.reshape(N * OH * OW, KH2 * KW2 * Cp2)\n",
    "\n",
    "        # 4) Gradientes de pesos y bias\n",
    "        #    dW_col: (KH*KW*C_in, F) = X_col^T @ dZ_col\n",
    "        dW_col = X_col.T.dot(dZ_col)\n",
    "        dW = dW_col.reshape(KH2, KW2, Cp2, self.filters)\n",
    "        db = xp.sum(dZ_col, axis=0)\n",
    "\n",
    "        # 5) Gradiente w.r.t. X_col: dX_col = dZ_col @ W_col^T  → (N*OH*OW, KH*KW*C_in)\n",
    "        C_from_W = self.W.shape[2]  # debe coincidir con Cp2\n",
    "        W_col = self.W.reshape(KH * KW * C_from_W, self.filters)  # (KH*KW*C_in, F)\n",
    "        dX_col = dZ_col.dot(W_col.T)  # (N*OH*OW, KH*KW*C_in)\n",
    "        dX_patches = dX_col.reshape(N, OH, OW, KH, KW, C_from_W)\n",
    "\n",
    "        # 6) Scatter dX_patches en dX_padded vía bucles sobre (KH, KW)\n",
    "        dX_padded = xp.zeros_like(self.X_padded)  # (N, H_padded, W_padded, C_in)\n",
    "        for p in range(KH):\n",
    "            for q in range(KW):\n",
    "                patch_grad = dX_patches[:, :, :, p, q, :]  # (N, OH, OW, C_in)\n",
    "                dX_padded[:, p:p + stride*OH:stride, q:q + stride*OW:stride, :] += patch_grad\n",
    "\n",
    "        # 7) Si hubo padding, recortar para obtener dX en forma (N, H, W, C_in)\n",
    "        if padding > 0:\n",
    "            dX = dX_padded[:, padding:-padding, padding:-padding, :]\n",
    "        else:\n",
    "            dX = dX_padded\n",
    "\n",
    "        # 8) Actualizar pesos y bias\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class MaxPool2D:\n",
    "    \"\"\"\n",
    "    Capa de Max Pooling 2D vectorizado usando sliding_window_view.\n",
    "    xp: numpy o cupy\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size=2, xp=np):\n",
    "        \"\"\"\n",
    "        pool_size: int (altura y ancho del bloque de pooling, stride = pool_size)\n",
    "        xp: módulo (np o cp)\n",
    "        \"\"\"\n",
    "        self.pool_size = pool_size\n",
    "        self.xp = xp\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward de MaxPool2D sin bucles anidados sobre posiciones.\n",
    "        X: (N, H, W, C)\n",
    "        Devuelve out: (N, H//ps, W//ps, C)\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        N, H, W, C = X.shape\n",
    "        PS = self.pool_size\n",
    "        OH = H // PS\n",
    "        OW = W // PS\n",
    "\n",
    "        # 1) Extraer parches con sliding_window_view sobre ejes (1,2)\n",
    "        #    Forma intermedia: (N, H-PS+1, W-PS+1, C, PS, PS)\n",
    "        if xp is np:\n",
    "            patches = np_sliding_window_view(X, (PS, PS), axis=(1, 2))\n",
    "        else:\n",
    "            patches = xp.lib.stride_tricks.sliding_window_view(\n",
    "                X, (PS, PS), axis=(1, 2)\n",
    "            )\n",
    "        # patches.shape = (N, H-PS+1, W-PS+1, C, PS, PS)\n",
    "        # Transponemos a (N, H-PS+1, W-PS+1, PS, PS, C)\n",
    "        patches = patches.transpose(0, 1, 2, 4, 5, 3)\n",
    "        # Ahora aplicamos stride = PS\n",
    "        patches = patches[:, ::PS, ::PS, :, :, :]  # → (N, OH, OW, PS, PS, C)\n",
    "\n",
    "        # 2) Calcular máximos y máscara\n",
    "        Np, OH2, OW2, p1, p2, Cp = patches.shape\n",
    "        patches_flat = patches.reshape(Np, OH2, OW2, p1*p2, Cp)  # (N, OH, OW, PS*PS, C)\n",
    "        max_vals = xp.max(patches_flat, axis=3)                # (N, OH, OW, C)\n",
    "        mask = (patches_flat == max_vals[:, :, :, None, :])    # (N, OH, OW, PS*PS, C)\n",
    "\n",
    "        # 3) Guardar para backward\n",
    "        self.X_shape = X.shape\n",
    "        self.mask = mask       # (N, OH, OW, PS*PS, C)\n",
    "        self.max_vals = max_vals\n",
    "        self.pool_size = PS\n",
    "\n",
    "        # 4) Devolver salida\n",
    "        return max_vals  # (N, OH, OW, C)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Retropropagación de MaxPool2D vectorizado.\n",
    "        dout: (N, OH, OW, C)\n",
    "        Devuelve dX: (N, H, W, C)\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        N, H, W, C = self.X_shape\n",
    "        PS = self.pool_size\n",
    "        OH = H // PS\n",
    "        OW = W // PS\n",
    "\n",
    "        # 1) Expandir dout para cada posición de PS*PS: (N, OH, OW, 1, 1, C)\n",
    "        dout_expanded = dout[:, :, :, None, None, :]  # (N, OH, OW, 1, 1, C)\n",
    "\n",
    "        # 2) mask: (N, OH, OW, PS*PS, C) → reordenar a (N, OH, OW, PS, PS, C)\n",
    "        mask = self.mask.reshape(N, OH, OW, PS, PS, C)  # (N, OH, OW, PS, PS, C)\n",
    "\n",
    "        # 3) Generar dX_patches: (N, OH, OW, PS, PS, C) = mask * dout_expanded\n",
    "        dX_patches = mask * dout_expanded  # broadcast sobre ejes 3 y 4\n",
    "\n",
    "        # 4) Scatter dX_patches en dX: (N, H, W, C)\n",
    "        dX = xp.zeros((N, H, W, C), dtype=dout.dtype)\n",
    "        for p in range(PS):\n",
    "            for q in range(PS):\n",
    "                dX[:, p:H:PS, q:W:PS, :] += dX_patches[:, :, :, p, q, :]\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    \"\"\"Capa Flatten\"\"\"\n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ.reshape(self.input_shape)\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    \"\"\"Capa fully connected (Dense) con xp\"\"\"\n",
    "    def __init__(self, units, input_dim, xp, activation='relu'):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.xp = xp\n",
    "        rng = xp.random if xp is not np else np.random\n",
    "        # Inicialización He en xp\n",
    "        self.W = rng.randn(input_dim, units, dtype=xp.float32) * xp.sqrt(2.0 / input_dim)\n",
    "        self.b = xp.zeros(units, dtype=xp.float32)\n",
    "\n",
    "    def forward(self, X):\n",
    "        xp = self.xp\n",
    "        self.X = X  # (batch, input_dim)\n",
    "        self.Z = xp.dot(X, self.W) + self.b  # (batch, units)\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            return xp.maximum(0, self.Z)\n",
    "        elif self.activation == 'softmax':\n",
    "            exp = xp.exp(self.Z - xp.max(self.Z, axis=1, keepdims=True))\n",
    "            return exp / xp.sum(exp, axis=1, keepdims=True)\n",
    "        else:\n",
    "            return self.Z\n",
    "\n",
    "    def backward(self, dA, lr):\n",
    "        xp = self.xp\n",
    "        batch_size = self.X.shape[0]\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            dZ = dA * (self.Z > 0)\n",
    "        else:\n",
    "            dZ = dA  # para softmax + crossentropy, dA ya es dZ\n",
    "\n",
    "        dW = xp.dot(self.X.T, dZ) / batch_size\n",
    "        db = xp.sum(dZ, axis=0) / batch_size\n",
    "        dX = xp.dot(dZ, self.W.T)\n",
    "\n",
    "        # Actualizar parámetros\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "# 4. Modelo CNN completo usando estas capas\n",
    "class CNN:\n",
    "    def __init__(self, xp):\n",
    "        \"\"\"\n",
    "        Arquitectura: Conv -> Pool -> Conv -> Pool -> Flatten -> Dense -> Output\n",
    "        xp: numpy o cupy\n",
    "        \"\"\"\n",
    "        self.xp = xp\n",
    "        # Input: (28, 28, 1)\n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=3, input_shape=(28, 28, 1),\n",
    "                            xp=xp, stride=1, padding=1, activation='relu')\n",
    "        self.pool1 = MaxPool2D(pool_size=2, xp=xp)\n",
    "        # Después de pool1: (14, 14, 32)\n",
    "        self.conv2 = Conv2D(filters=64, kernel_size=3, input_shape=(14, 14, 32),\n",
    "                            xp=xp, stride=1, padding=1, activation='relu')\n",
    "        self.pool2 = MaxPool2D(pool_size=2, xp=xp)\n",
    "        # Después de pool2: (7, 7, 64)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(units=128, input_dim=7*7*64, xp=xp, activation='relu')\n",
    "        self.output = Dense(units=10, input_dim=128, xp=xp, activation='softmax')\n",
    "\n",
    "    def forward(self, X):\n",
    "        xp = self.xp\n",
    "        X = self.conv1.forward(X)   # (N, 28, 28, 32)\n",
    "        X = self.pool1.forward(X)   # (N, 14, 14, 32)\n",
    "        X = self.conv2.forward(X)   # (N, 14, 14, 64)\n",
    "        X = self.pool2.forward(X)   # (N, 7, 7, 64)\n",
    "        X = self.flatten.forward(X) # (N, 7*7*64)\n",
    "        X = self.dense.forward(X)   # (N, 128)\n",
    "        return self.output.forward(X)  # (N, 10)\n",
    "\n",
    "    def backward(self, dZ, lr):\n",
    "        # dZ aquí es gradiente de la pérdida w.r.t. salidas softmax\n",
    "        dZ = self.output.backward(dZ, lr)  # (N, 128)\n",
    "        dZ = self.dense.backward(dZ, lr)   # (N, 7*7*64)\n",
    "        dZ = self.flatten.backward(dZ)     # (N, 7, 7, 64)\n",
    "        dZ = self.pool2.backward(dZ)       # (N, 14, 14, 64)\n",
    "        dZ = self.conv2.backward(dZ, lr)   # (N, 14, 14, 32)\n",
    "        dZ = self.pool1.backward(dZ)       # (N, 28, 28, 32)\n",
    "        dZ = self.conv1.backward(dZ, lr)   # (N, 28, 28, 1)\n",
    "        # No devolvemos nada; se actualizan parámetros en cada capa\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss.\n",
    "        y_true: one‐hot (N, 10)\n",
    "        y_pred: (N, 10)\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        eps = 1e-15\n",
    "        y_pred_clip = xp.clip(y_pred, eps, 1.0 - eps)\n",
    "        return -xp.mean(xp.sum(y_true * xp.log(y_pred_clip), axis=1))\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, lr=0.001):\n",
    "        xp = self.xp\n",
    "        # One‐hot encoding en xp\n",
    "        y_train_oh = xp.eye(10, dtype=xp.float32)[y_train]\n",
    "        y_val_oh = xp.eye(10, dtype=xp.float32)[y_val]\n",
    "\n",
    "        history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "        epoch_pbar = tqdm(range(epochs), desc='Entrenando', unit='epoch', position=0)\n",
    "\n",
    "        for epoch in epoch_pbar:\n",
    "            epoch_loss = 0.0\n",
    "            epoch_acc = 0.0\n",
    "            num_batches = (len(X_train) + batch_size - 1) // batch_size\n",
    "\n",
    "            batch_pbar = tqdm(range(0, len(X_train), batch_size),\n",
    "                              desc=f'Época {epoch+1}/{epochs}',\n",
    "                              unit='batch', position=1, leave=False, total=num_batches)\n",
    "\n",
    "            for i in batch_pbar:\n",
    "                X_batch = X_train[i:i+batch_size]    # NumPy array\n",
    "                y_batch = y_train_oh[i:i+batch_size] # NumPy array, one‐hot\n",
    "\n",
    "                # Mover a GPU si xp == cp\n",
    "                if xp is cp:\n",
    "                    Xb = cp.asarray(X_batch)\n",
    "                    yb = cp.asarray(y_batch)\n",
    "                else:\n",
    "                    Xb = X_batch\n",
    "                    yb = y_batch\n",
    "\n",
    "                # Forward\n",
    "                y_pred = self.forward(Xb)  # (batch, 10)\n",
    "                loss = self.compute_loss(yb, y_pred)\n",
    "\n",
    "                # Gradiente de cross‐entropy con softmax\n",
    "                dZ = (y_pred - yb) / Xb.shape[0]\n",
    "\n",
    "                # Backward\n",
    "                self.backward(dZ, lr)\n",
    "\n",
    "                # Calcular accuracy de batch (pasar a NumPy para sklearn)\n",
    "                if xp is cp:\n",
    "                    y_pred_np = cp.asnumpy(y_pred)\n",
    "                    yb_np = cp.asnumpy(yb)\n",
    "                else:\n",
    "                    y_pred_np = y_pred\n",
    "                    yb_np = yb\n",
    "\n",
    "                batch_acc = accuracy_score(\n",
    "                    np.argmax(yb_np, axis=1),\n",
    "                    np.argmax(y_pred_np, axis=1)\n",
    "                )\n",
    "                epoch_loss += float(loss) * Xb.shape[0]\n",
    "                epoch_acc += batch_acc * Xb.shape[0]\n",
    "\n",
    "                batch_pbar.set_postfix({\n",
    "                    'loss': f'{float(loss):.4f}',\n",
    "                    'acc': f'{batch_acc:.4f}'\n",
    "                })\n",
    "\n",
    "            batch_pbar.close()\n",
    "\n",
    "            # Validación\n",
    "            if xp is cp:\n",
    "                Xv = cp.asarray(X_val)\n",
    "                yv = cp.asarray(y_val_oh)\n",
    "                val_pred = self.forward(Xv)\n",
    "                val_pred_np = cp.asnumpy(val_pred)\n",
    "                y_val_oh_np = cp.asnumpy(y_val_oh)\n",
    "            else:\n",
    "                val_pred = self.forward(X_val)\n",
    "                val_pred_np = val_pred\n",
    "                y_val_oh_np = y_val_oh\n",
    "\n",
    "            # Calcular val_loss\n",
    "            if xp is cp:\n",
    "                val_loss = self.compute_loss(yv, val_pred)\n",
    "                val_loss = float(cp.asnumpy(val_loss))\n",
    "            else:\n",
    "                val_loss = float(self.compute_loss(y_val_oh, val_pred))\n",
    "\n",
    "            val_acc = accuracy_score(y_val, np.argmax(val_pred_np, axis=1))\n",
    "\n",
    "            avg_loss = epoch_loss / len(X_train)\n",
    "            avg_acc = epoch_acc / len(X_train)\n",
    "\n",
    "            history['loss'].append(avg_loss)\n",
    "            history['acc'].append(avg_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            epoch_pbar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'acc': f'{avg_acc:.4f}',\n",
    "                'val_loss': f'{val_loss:.4f}',\n",
    "                'val_acc': f'{val_acc:.4f}'\n",
    "            })\n",
    "\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, X, y, batch_size_eval=256):\n",
    "        \"\"\"\n",
    "        Evalúa en modo GPU/CPU usando batches pequeños para no quedarnos OOM.\n",
    "        X: numpy.ndarray con todas las imágenes de test (shape: [N, 28, 28, 1])\n",
    "        y: numpy.ndarray con etiquetas (shape: [N])\n",
    "        batch_size_eval: cuántas imágenes procesar en GPU a la vez\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "\n",
    "        all_preds = []   # aquí iremos guardando los predichos de cada batch\n",
    "\n",
    "        N = X.shape[0]\n",
    "        for i in range(0, N, batch_size_eval):\n",
    "            X_batch = X[i : i + batch_size_eval]\n",
    "            # Convertimos esa porción a GPU (si xp=cp) o la dejamos como está (si xp=np)\n",
    "            if xp is cp:\n",
    "                Xb = cp.asarray(X_batch)\n",
    "            else:\n",
    "                Xb = X_batch\n",
    "\n",
    "            # Forward en ese batch\n",
    "            y_pred_batch = self.forward(Xb)  # (batch_size_eval, 10)\n",
    "\n",
    "            # Extraemos las etiquetas predichas (argmax) y pasamos a NumPy\n",
    "            if xp is cp:\n",
    "                y_pred_cpu = cp.asnumpy(xp.argmax(y_pred_batch, axis=1))\n",
    "            else:\n",
    "                y_pred_cpu = np.argmax(y_pred_batch, axis=1)\n",
    "\n",
    "            all_preds.append(y_pred_cpu)\n",
    "\n",
    "            # Opcionalmente: liberar memoria pool de CuPy\n",
    "            if xp is cp:\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "        # Concatenamos todas las predicciones\n",
    "        y_pred_labels = np.concatenate(all_preds, axis=0)  # forma (N,)\n",
    "\n",
    "        # Ahora podemos calcular métricas en CPU\n",
    "        print(\"\\nExactitud:\", accuracy_score(y, y_pred_labels))\n",
    "        print(\"\\nReporte de Clasificación:\")\n",
    "        print(classification_report(y, y_pred_labels))\n",
    "\n",
    "        cm = confusion_matrix(y, y_pred_labels)\n",
    "        plot_combined_confusion_matrix(cm, classes=[str(i) for i in range(10)])\n",
    "\n",
    "\n",
    "# 5. Función main adaptada\n",
    "def main(device='gpu'):\n",
    "    \"\"\"\n",
    "    device: 'gpu' o 'cpu'. Si pides 'gpu' pero no hay CuPy, cae a 'cpu'.\n",
    "    \"\"\"\n",
    "    xp = get_xp(device)\n",
    "    if device == 'gpu' and xp is np:\n",
    "        print(\"No se detectó CuPy/GPU; se usará CPU (NumPy).\")\n",
    "    elif xp is cp:\n",
    "        print(\"Usando GPU (CuPy) para operaciones numéricas.\")\n",
    "    else:\n",
    "        print(\"Usando CPU (NumPy) para operaciones numéricas.\")\n",
    "\n",
    "    # 1. Cargar datos (siempre en CPU)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_mnist_data()\n",
    "\n",
    "    # 2. Crear y entrenar modelo\n",
    "    model = CNN(xp=xp)\n",
    "    history = model.train(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=35,\n",
    "        batch_size=64,\n",
    "        lr=0.001\n",
    "    )\n",
    "\n",
    "    # 3. Graficar curvas de pérdida y accuracy (convertir a NumPy si xp es CuPy)\n",
    "    loss_train = np.array(history['loss'])\n",
    "    loss_val = np.array(history['val_loss'])\n",
    "    acc_train = np.array(history['acc'])\n",
    "    acc_val = np.array(history['val_acc'])\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_train, label='Train')\n",
    "    plt.plot(loss_val, label='Validation')\n",
    "    plt.title('Pérdida por Época')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(acc_train, label='Train')\n",
    "    plt.plot(acc_val, label='Validation')\n",
    "    plt.title('Exactitud por Época')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Exactitud')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Evaluación en test\n",
    "    print(\"\\nEvaluación en Test:\")\n",
    "    model.evaluate(X_test, y_test)\n",
    "\n",
    "    # 5. Limpiar para liberar GPU\n",
    "    if xp is cp:\n",
    "        # 5.1. Eliminar referencias a objetos grandes\n",
    "        del model\n",
    "        del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "        del history, loss_train, loss_val, acc_train, acc_val\n",
    "\n",
    "        # 5.2. Forzar recolección de basura de Python\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        # 5.3. Liberar los memory pools de CuPy\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "\n",
    "        print(\"Memoria GPU liberada.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(device='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870d3f3-5e87-4972-838c-46a76ce08984",
   "metadata": {},
   "source": [
    "# Red Neuronal Convolucional para Clasificación de MNIST con apoyo para GPU y visualización de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1df23b-a400-4de0-b850-90792c489f70",
   "metadata": {},
   "source": [
    "El siguiente ejemplo añade más complejidad, se visualiza qué podría estar aprendiendo la red neuronal. Estudie el código y ejecútelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4275d-b240-4afc-905c-ea1bc7f94707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Detectar dispositivo y cargar xp = cupy o numpy\n",
    "try:\n",
    "    import cupy as cp\n",
    "    _has_cupy = True\n",
    "except ImportError:\n",
    "    cp = None\n",
    "    _has_cupy = False\n",
    "\n",
    "import numpy as np  # Para fallback cuando no haya CuPy\n",
    "from numpy.lib.stride_tricks import sliding_window_view as np_sliding_window_view\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_xp(device):\n",
    "    \"\"\"\n",
    "    Devuelve el módulo xp: cp si device='gpu' y CuPy está disponible,\n",
    "    en caso contrario NumPy.\n",
    "    \"\"\"\n",
    "    device = device.lower()\n",
    "    if device == 'gpu' and _has_cupy:\n",
    "        return cp\n",
    "    else:\n",
    "        return np\n",
    "\n",
    "def to_cpu(x, xp):\n",
    "    \"\"\"\n",
    "    Si xp es CuPy, convierte x a NumPy; si xp es NumPy, lo devuelve igual.\n",
    "    \"\"\"\n",
    "    if xp is cp:\n",
    "        return cp.asnumpy(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# 1. Función para graficar matriz de confusión (convierte a NumPy si es CuPy)\n",
    "def plot_combined_confusion_matrix(cm, classes, title='Matriz de Confusión'):\n",
    "    \"\"\"Matriz combinada con valores absolutos y relativos\"\"\"\n",
    "    if _has_cupy and isinstance(cm, cp.ndarray):\n",
    "        cm_np = cp.asnumpy(cm)\n",
    "    else:\n",
    "        cm_np = cm\n",
    "\n",
    "    cm_norm = cm_np.astype('float') / cm_np.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "    plt.title(title, pad=20, fontsize=14)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "\n",
    "    thresh = 0.5\n",
    "    for i, j in np.ndindex(cm_np.shape):\n",
    "        plt.text(j, i-0.15, f\"{cm_np[i, j]}\",\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm_norm[i, j] > thresh else 'black',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.text(j, i+0.15, f\"({cm_norm[i, j]:.1%})\",\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm_norm[i, j] > thresh else 'black',\n",
    "                 fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta Verdadera', fontsize=12)\n",
    "    plt.xlabel('Predicción', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# 2. Cargar y preparar datos de MNIST (siempre en CPU, porque sklearn retorna NumPy)\n",
    "def load_mnist_data(test_size=0.2, val_size=0.1, random_state=42, xp=np):\n",
    "    \"\"\"\n",
    "    Carga MNIST y divide en train/val/test usando NumPy (o convierte a xp si es CuPy).\n",
    "    \"\"\"\n",
    "    print(\"Cargando MNIST...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "    # Normalizar y redimensionar\n",
    "    X = X.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "    # Dividir en train+val y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    # Dividir train en train y val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, stratify=y_train, random_state=random_state)\n",
    "\n",
    "    # Si xp es CuPy, convertimos\n",
    "    if xp is not np:\n",
    "        X_train = xp.asarray(X_train)\n",
    "        X_val   = xp.asarray(X_val)\n",
    "        X_test  = xp.asarray(X_test)\n",
    "        y_train = xp.asarray(y_train)\n",
    "        y_val   = xp.asarray(y_val)\n",
    "        y_test  = xp.asarray(y_test)\n",
    "\n",
    "    print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# 3. Clases de capas usando im2col / sliding_window_view con xp (NumPy o CuPy),\n",
    "#    pero extendidas para almacenar la última salida (feature maps).\n",
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    Capa convolucional 2D usando im2col + matmul (BLAS/cuBLAS) y sliding_window_view.\n",
    "    xp: numpy o cupy. Además guarda last_output para poder visualizar feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, kernel_size, input_shape, xp, stride=1, padding=1, activation='relu', name=\"conv\"):\n",
    "        \"\"\"\n",
    "        filters: número de filtros (F)\n",
    "        kernel_size: entero (KH = KW)\n",
    "        input_shape: tupla (H, W, C) de la entrada esperada (solo para inicialización)\n",
    "        xp: módulo (np o cp)\n",
    "        stride: entero (por defecto 1)\n",
    "        padding: entero (por defecto 1 para 'same')\n",
    "        activation: 'relu' o None\n",
    "        name: nombre de la capa (solo para identificación al visualizar)\n",
    "        \"\"\"\n",
    "        self.filters = filters\n",
    "        self.KH = kernel_size\n",
    "        self.KW = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.xp = xp\n",
    "        self.name = name\n",
    "\n",
    "        # Inicializamos pesos con He, a partir de input_shape[-1]\n",
    "        H_in, W_in, C_in = input_shape\n",
    "        fan_in = kernel_size * kernel_size * C_in\n",
    "        rng = xp.random if xp is not np else np.random\n",
    "        self.W = rng.randn(self.KH, self.KW, C_in, filters, dtype=xp.float32) * xp.sqrt(2.0 / fan_in)\n",
    "        self.b = xp.zeros(filters, dtype=xp.float32)\n",
    "\n",
    "        # Para almacenar última salida y última entrada (si quieres usarla)\n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass vectorizado con im2col / sliding_window_view.\n",
    "        X: (N, H, W, C) — numpy.ndarray o cupy.ndarray\n",
    "        Devuelve: (N, OH, OW, filters)\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        N, H, W_in, C_in = X.shape\n",
    "        self.last_input = X  # Guardamos la entrada para poder ver cómo reacciona.\n",
    "\n",
    "        # 1) Padding si es necesario\n",
    "        if self.padding > 0:\n",
    "            X_padded = xp.pad(\n",
    "                X,\n",
    "                ((0, 0),\n",
    "                 (self.padding, self.padding),\n",
    "                 (self.padding, self.padding),\n",
    "                 (0, 0)),\n",
    "                mode='constant',\n",
    "                constant_values=0\n",
    "            )\n",
    "        else:\n",
    "            X_padded = X\n",
    "\n",
    "        # 2) Calcular dimensiones de salida\n",
    "        H_padded = H + 2*self.padding\n",
    "        W_padded = W_in + 2*self.padding\n",
    "        OH = (H_padded - self.KH) // self.stride + 1\n",
    "        OW = (W_padded - self.KW) // self.stride + 1\n",
    "        self.OH, self.OW = OH, OW\n",
    "\n",
    "        # 3) Extraer parches con sliding_window_view sobre ejes (1,2)\n",
    "        #    NumPy/CuPy devuelven shape = (N, H_padded-KH+1, W_padded-KW+1, C_in, KH, KW)\n",
    "        if xp is np:\n",
    "            patches = np_sliding_window_view(X_padded, (self.KH, self.KW), axis=(1, 2))\n",
    "        else:\n",
    "            patches = xp.lib.stride_tricks.sliding_window_view(\n",
    "                X_padded, (self.KH, self.KW), axis=(1, 2)\n",
    "            )\n",
    "        # Ahora: patches.shape = (N, OH+stride_offset, OW+stride_offset, C_in, KH, KW).\n",
    "        # Queremos pasar a (N, OH, OW, KH, KW, C_in), así que transponemos:\n",
    "        patches = patches.transpose(0, 1, 2, 4, 5, 3)  # → (N, OH, OW, KH, KW, C_in)\n",
    "\n",
    "        # Aplicamos el stride recortando según stride:\n",
    "        patches = patches[:, ::self.stride, ::self.stride, :, :, :]  # → (N, OH, OW, KH, KW, C_in)\n",
    "\n",
    "        # Guardamos los parches para backward (no para visualizar, pero para retroprop).\n",
    "        self.X_padded = X_padded\n",
    "        self.X_patches = patches  # (N, OH, OW, KH, KW, C_in)\n",
    "\n",
    "        # 4) Aplanar parches para matmul: (N*OH*OW, KH*KW*C_in)\n",
    "        Np, OH, OW, KH, KW, Cp = patches.shape\n",
    "        X_col = patches.reshape(Np, OH, OW, KH*KW*Cp)      # (N, OH, OW, KH*KW*C_in)\n",
    "        X_col = X_col.transpose(0, 1, 2, 3).reshape(N*OH*OW, KH*KW*Cp)\n",
    "\n",
    "        # 5) Aplanar pesos W: (KH, KW, C_in, F) → (KH*KW*C_in, F)\n",
    "        C_from_W = self.W.shape[2]  # este debe coincidir con Cp\n",
    "        W_col = self.W.reshape(self.KH * self.KW * C_from_W, self.filters)\n",
    "\n",
    "        # 6) Matmul grande + bias  → out_col: (N*OH*OW, F)\n",
    "        out_col = X_col.dot(W_col) + self.b.reshape(1, self.filters)\n",
    "\n",
    "        # 7) Reconstruir salida: (N, OH, OW, filters)\n",
    "        out = out_col.reshape(N, OH, OW, self.filters)\n",
    "\n",
    "        # 8) Aplicar activación\n",
    "        if self.activation == 'relu':\n",
    "            self.out = xp.maximum(0, out)\n",
    "        else:\n",
    "            self.out = out\n",
    "\n",
    "        # Guardamos esta salida como “feature maps” para visualizar después\n",
    "        self.last_output = self.out\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dZ, lr):\n",
    "        \"\"\"\n",
    "        Retropropagación usando operaciones vectorizadas + scatter.\n",
    "        dZ: (N, OH, OW, F)\n",
    "        lr: learning rate (float)\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        N, H, W_in, C_in = self.last_input.shape\n",
    "        KH, KW = self.KH, self.KW\n",
    "        stride, padding = self.stride, self.padding\n",
    "        OH, OW = self.OH, self.OW\n",
    "\n",
    "        # 1) Si hay ReLU, enmascarar gradiente\n",
    "        if self.activation == 'relu':\n",
    "            dZ = dZ * (self.last_output > 0)\n",
    "\n",
    "        # 2) Convertir dZ a dZ_col: (N*OH*OW, F)\n",
    "        dZ_col = dZ.reshape(N * OH * OW, self.filters)\n",
    "\n",
    "        # 3) Reconstruir X_col: (N*OH*OW, KH*KW*C_in) usando los parches guardados\n",
    "        patches = self.X_patches  # (N, OH, OW, KH, KW, C_in)\n",
    "        _, _, _, KH2, KW2, Cp2 = patches.shape  # Cp2 == C_in\n",
    "        X_col = patches.reshape(N, OH, OW, KH2 * KW2 * Cp2).transpose(0, 1, 2, 3)\n",
    "        X_col = X_col.reshape(N * OH * OW, KH2 * KW2 * Cp2)\n",
    "\n",
    "        # 4) Gradientes de pesos y bias\n",
    "        dW_col = X_col.T.dot(dZ_col)  # (KH*KW*C_in, F)\n",
    "        dW = dW_col.reshape(KH2, KW2, Cp2, self.filters)\n",
    "        db = xp.sum(dZ_col, axis=0)\n",
    "\n",
    "        # 5) Gradiente w.r.t. X_col: dX_col = dZ_col @ W_col^T  → (N*OH*OW, KH*KW*C_in)\n",
    "        C_from_W = self.W.shape[2]\n",
    "        W_col = self.W.reshape(KH * KW * C_from_W, self.filters)  # (KH*KW*C_in, F)\n",
    "        dX_col = dZ_col.dot(W_col.T)  # → (N*OH*OW, KH*KW*C_in)\n",
    "        dX_patches = dX_col.reshape(N, OH, OW, KH, KW, C_from_W)\n",
    "\n",
    "        # 6) Scatter dX_patches en dX_padded vía 2 bucles sobre (KH, KW)\n",
    "        dX_padded = xp.zeros_like(self.X_padded)  # (N, H_padded, W_padded, C_in)\n",
    "        for p in range(KH):\n",
    "            for q in range(KW):\n",
    "                patch_grad = dX_patches[:, :, :, p, q, :]  # (N, OH, OW, C_in)\n",
    "                dX_padded[:, p:p + stride*OH:stride, q:q + stride*OW:stride, :] += patch_grad\n",
    "\n",
    "        # 7) Si hubo padding, recortar para obtener dX en forma (N, H, W, C_in)\n",
    "        if padding > 0:\n",
    "            dX = dX_padded[:, padding:-padding, padding:-padding, :]\n",
    "        else:\n",
    "            dX = dX_padded\n",
    "\n",
    "        # 8) Actualizar pesos y bias\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def get_filters(self):\n",
    "        \"\"\"Retorna los filtros aprendidos (CPU numpy array) para visualización.\"\"\"\n",
    "        return to_cpu(self.W, self.xp)\n",
    "\n",
    "    def get_feature_maps(self, sample_idx=0):\n",
    "        \"\"\"\n",
    "        Retorna los mapas de características (feature maps) de la última pasada\n",
    "        para la muestra sample_idx. Entrega un NumPy array.\n",
    "        Forma: (OH, OW, filters)\n",
    "        \"\"\"\n",
    "        if self.last_output is None:\n",
    "            return None\n",
    "        fmap = self.last_output[sample_idx]  # (OH, OW, filters) en xp\n",
    "        return to_cpu(fmap, self.xp)\n",
    "\n",
    "class MaxPool2D:\n",
    "    \"\"\"\n",
    "    Capa de Max Pooling 2D vectorizado usando sliding_window_view.\n",
    "    xp: numpy o cupy\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size=2, xp=np, name=\"pool\"):\n",
    "        \"\"\"\n",
    "        pool_size: int (altura y ancho del bloque de pooling, stride = pool_size)\n",
    "        xp: módulo (np o cp)\n",
    "        name: nombre de la capa (solo para identificación)\n",
    "        \"\"\"\n",
    "        self.pool_size = pool_size\n",
    "        self.xp = xp\n",
    "        self.name = name\n",
    "\n",
    "        # Para visualizar feature maps después del pool\n",
    "        self.last_output = None\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward de MaxPool2D sin bucles anidados sobre posiciones (en ejes H,W).\n",
    "        X: (N, H, W, C)\n",
    "        Devuelve out: (N, H//ps, W//ps, C) y guarda last_output\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        N, H, W, C = X.shape\n",
    "        PS = self.pool_size\n",
    "        OH = H // PS\n",
    "        OW = W // PS\n",
    "\n",
    "        # 1) Extraer parches con sliding_window_view sobre ejes (1,2)\n",
    "        #    → (N, H-PS+1, W-PS+1, C, PS, PS)\n",
    "        if xp is np:\n",
    "            patches = np_sliding_window_view(X, (PS, PS), axis=(1, 2))\n",
    "        else:\n",
    "            patches = xp.lib.stride_tricks.sliding_window_view(\n",
    "                X, (PS, PS), axis=(1, 2)\n",
    "            )\n",
    "        # patches.shape = (N, H-PS+1, W-PS+1, C, PS, PS)\n",
    "        # Transponemos a (N, H-PS+1, W-PS+1, PS, PS, C)\n",
    "        patches = patches.transpose(0, 1, 2, 4, 5, 3)\n",
    "        # Aplicamos stride = PS\n",
    "        patches = patches[:, ::PS, ::PS, :, :, :]  # → (N, OH, OW, PS, PS, C)\n",
    "\n",
    "        # 2) Calcular máximos y máscara\n",
    "        Np, OH2, OW2, p1, p2, Cp = patches.shape\n",
    "        patches_flat = patches.reshape(Np, OH2, OW2, p1*p2, Cp)  # (N, OH, OW, PS*PS, C)\n",
    "        max_vals = xp.max(patches_flat, axis=3)                # (N, OH, OW, C)\n",
    "        mask = (patches_flat == max_vals[:, :, :, None, :])    # (N, OH, OW, PS*PS, C)\n",
    "\n",
    "        # 3) Guardar para backward\n",
    "        self.last_output = max_vals\n",
    "        self.mask = mask\n",
    "        self.X_shape = X.shape\n",
    "\n",
    "        # 4) Devolver salida\n",
    "        return max_vals  # (N, OH, OW, C)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Retropropagación de MaxPool2D vectorizado.\n",
    "        dZ: (N, OH, OW, C)\n",
    "        Devuelve dX: (N, H, W, C)\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        N, H, W, C = self.X_shape\n",
    "        PS = self.pool_size\n",
    "        OH = H // PS\n",
    "        OW = W // PS\n",
    "\n",
    "        # 1) Expandir dout para cada posición de PS*PS: (N, OH, OW, 1, 1, C)\n",
    "        dout_expanded = dZ[:, :, :, None, None, :]  # (N, OH, OW, 1, 1, C)\n",
    "\n",
    "        # 2) mask: (N, OH, OW, PS*PS, C) → (N, OH, OW, PS, PS, C)\n",
    "        mask = self.mask.reshape(N, OH, OW, PS, PS, C)\n",
    "\n",
    "        # 3) Generar dX_patches: (N, OH, OW, PS, PS, C) = mask * dout_expanded\n",
    "        dX_patches = mask * dout_expanded  # broadcast sobre ejes 3 y 4\n",
    "\n",
    "        # 4) Scatter dX_patches en dX: (N, H, W, C)\n",
    "        dX = xp.zeros((N, H, W, C), dtype=dZ.dtype)\n",
    "        for p in range(PS):\n",
    "            for q in range(PS):\n",
    "                dX[:, p:H:PS, q:W:PS, :] += dX_patches[:, :, :, p, q, :]\n",
    "\n",
    "        return dX\n",
    "\n",
    "class Flatten:\n",
    "    \"\"\"Capa Flatten\"\"\"\n",
    "    def __init__(self, xp=np, name=\"flatten\"):\n",
    "        self.xp = xp\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ.reshape(self.input_shape)\n",
    "\n",
    "class Dense:\n",
    "    \"\"\"Capa Fully Connected\"\"\"\n",
    "    def __init__(self, units, input_dim, xp=np, activation='relu', name=\"dense\"):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.xp = xp\n",
    "        self.name = name\n",
    "        rng = xp.random if xp is not np else np.random\n",
    "        self.W = rng.randn(input_dim, units, dtype=xp.float32) * xp.sqrt(2./input_dim)\n",
    "        self.b = xp.zeros(units, dtype=xp.float32)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X  # (batch, input_dim)\n",
    "        xp = self.xp\n",
    "        self.Z = xp.dot(X, self.W) + self.b  # (batch, units)\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            return xp.maximum(0, self.Z)\n",
    "        elif self.activation == 'softmax':\n",
    "            exp = xp.exp(self.Z - xp.max(self.Z, axis=1, keepdims=True))\n",
    "            return exp / xp.sum(exp, axis=1, keepdims=True)\n",
    "        else:\n",
    "            return self.Z\n",
    "\n",
    "    def backward(self, dA, lr):\n",
    "        xp = self.xp\n",
    "        batch_size = self.X.shape[0]\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            dZ = dA * (self.Z > 0)\n",
    "        else:\n",
    "            dZ = dA  # para softmax + crossentropy, dA ya es dZ\n",
    "\n",
    "        dW = xp.dot(self.X.T, dZ) / batch_size\n",
    "        db = xp.sum(dZ, axis=0) / batch_size\n",
    "        dX = xp.dot(dZ, self.W.T)\n",
    "\n",
    "        # Actualizar parámetros\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "\n",
    "        return dX\n",
    "\n",
    "# 4. Modelo CNN con capacidades de visualización\n",
    "class VisualizableCNN:\n",
    "    def __init__(self, xp=np):\n",
    "        \"\"\"\n",
    "        Arquitectura: Conv -> Pool -> Conv -> Pool -> Flatten -> Dense -> Output,\n",
    "        con nombres y capacidad de visualización.\n",
    "        \"\"\"\n",
    "        self.xp = xp\n",
    "        # Input: (28, 28, 1)\n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=3, input_shape=(28, 28, 1),\n",
    "                            xp=xp, stride=1, padding=1, activation='relu', name=\"conv1\")\n",
    "        self.pool1 = MaxPool2D(pool_size=2, xp=xp, name=\"pool1\")\n",
    "        # Después de pool1: (14, 14, 32)\n",
    "        self.conv2 = Conv2D(filters=64, kernel_size=3, input_shape=(14, 14, 32),\n",
    "                            xp=xp, stride=1, padding=1, activation='relu', name=\"conv2\")\n",
    "        self.pool2 = MaxPool2D(pool_size=2, xp=xp, name=\"pool2\")\n",
    "        # Después de pool2: (7, 7, 64)\n",
    "        self.flatten = Flatten(xp=xp, name=\"flatten\")\n",
    "        self.dense = Dense(units=128, input_dim=7*7*64, xp=xp, activation='relu', name=\"dense\")\n",
    "        self.output = Dense(units=10, input_dim=128, xp=xp, activation='softmax', name=\"output\")\n",
    "\n",
    "        # Para iterar fácilmente sobre las capas convolucionales:\n",
    "        self.conv_layers = [self.conv1, self.conv2]\n",
    "\n",
    "    def forward(self, X):\n",
    "        xp = self.xp\n",
    "        X = self.conv1.forward(X)\n",
    "        X = self.pool1.forward(X)\n",
    "        X = self.conv2.forward(X)\n",
    "        X = self.pool2.forward(X)\n",
    "        X = self.flatten.forward(X)\n",
    "        X = self.dense.forward(X)\n",
    "        return self.output.forward(X)\n",
    "\n",
    "    def backward(self, dZ, lr):\n",
    "        # dZ aquí es gradiente de la pérdida w.r.t. salidas softmax\n",
    "        dZ = self.output.backward(dZ, lr)\n",
    "        dZ = self.dense.backward(dZ, lr)\n",
    "        dZ = self.flatten.backward(dZ)\n",
    "        dZ = self.pool2.backward(dZ)\n",
    "        dZ = self.conv2.backward(dZ, lr)\n",
    "        dZ = self.pool1.backward(dZ)\n",
    "        dZ = self.conv1.backward(dZ, lr)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss.\n",
    "        y_true: one‐hot (N, 10)\n",
    "        y_pred: (N, 10)\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        eps = 1e-15\n",
    "        y_pred_clip = xp.clip(y_pred, eps, 1.0 - eps)\n",
    "        return -xp.mean(xp.sum(y_true * xp.log(y_pred_clip), axis=1))\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, lr=0.001):\n",
    "        \"\"\"\n",
    "        Entrenamiento de la red. Retorna un diccionario con historia de loss/accuracy.\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "\n",
    "        # One‐hot encoding en xp\n",
    "        if xp is np:\n",
    "            y_train_oh = np.eye(10, dtype=np.float32)[to_cpu(y_train, xp)]\n",
    "            y_val_oh   = np.eye(10, dtype=np.float32)[to_cpu(y_val, xp)]\n",
    "        else:\n",
    "            y_train_cpu = to_cpu(y_train, xp)\n",
    "            y_val_cpu   = to_cpu(y_val, xp)\n",
    "            y_train_oh = xp.asarray(np.eye(10, dtype=np.float32)[y_train_cpu])\n",
    "            y_val_oh   = xp.asarray(np.eye(10, dtype=np.float32)[y_val_cpu])\n",
    "\n",
    "        history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "\n",
    "        for epoch in tqdm(range(epochs), desc='Entrenando', unit='epoch'):\n",
    "            epoch_loss = 0.0\n",
    "            epoch_acc = 0.0\n",
    "\n",
    "            # Shuffle manual si quieres:  \n",
    "            # idx = xp.random.permutation(len(X_train)); X_train = X_train[idx]; y_train_oh = y_train_oh[idx]\n",
    "\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                X_batch = X_train[i:i+batch_size]\n",
    "                y_batch = y_train_oh[i:i+batch_size]\n",
    "\n",
    "                # Forward\n",
    "                y_pred = self.forward(X_batch)\n",
    "                loss = self.compute_loss(y_batch, y_pred)\n",
    "\n",
    "                # Gradiente de cross‐entropy con softmax\n",
    "                dZ = (y_pred - y_batch) / X_batch.shape[0]\n",
    "\n",
    "                # Backward\n",
    "                self.backward(dZ, lr)\n",
    "\n",
    "                # Calcular accuracy de batch (si xp es CuPy, convertimos a CPU para sklearn)\n",
    "                if xp is cp:\n",
    "                    y_pred_cpu = to_cpu(xp.argmax(y_pred, axis=1), xp)\n",
    "                    y_batch_cpu = to_cpu(xp.argmax(y_batch, axis=1), xp)\n",
    "                else:\n",
    "                    y_pred_cpu = np.argmax(y_pred, axis=1)\n",
    "                    y_batch_cpu = np.argmax(y_batch, axis=1)\n",
    "\n",
    "                batch_acc = accuracy_score(y_batch_cpu, y_pred_cpu)\n",
    "                epoch_loss += float(to_cpu(loss, xp)) * X_batch.shape[0]\n",
    "                epoch_acc += batch_acc * X_batch.shape[0]\n",
    "\n",
    "            # Validación\n",
    "            val_pred = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val_oh, val_pred)\n",
    "\n",
    "            if xp is cp:\n",
    "                val_pred_cpu = to_cpu(xp.argmax(val_pred, axis=1), xp)\n",
    "                y_val_cpu = to_cpu(y_val, xp)\n",
    "                val_acc = accuracy_score(y_val_cpu, val_pred_cpu)\n",
    "            else:\n",
    "                val_pred_cpu = np.argmax(val_pred, axis=1)\n",
    "                val_acc = accuracy_score(y_val, val_pred_cpu)\n",
    "\n",
    "            history['loss'].append(epoch_loss / len(X_train))\n",
    "            history['acc'].append(epoch_acc / len(X_train))\n",
    "            history['val_loss'].append(float(to_cpu(val_loss, xp)))\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, X, y, batch_size_eval=256):\n",
    "        \"\"\"\n",
    "        Evalúa en modo GPU/CPU usando batches pequeños para no quedarnos OOM.\n",
    "        X: numpy.ndarray con todas las imágenes de test (shape: [N, 28, 28, 1])\n",
    "        y: numpy.ndarray con etiquetas (shape: [N])\n",
    "        batch_size_eval: cuántas imágenes procesar en GPU a la vez\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "\n",
    "        all_preds = []   # aquí iremos guardando los predichos de cada batch\n",
    "\n",
    "        N = X.shape[0]\n",
    "        for i in range(0, N, batch_size_eval):\n",
    "            X_batch = X[i : i + batch_size_eval]\n",
    "            # Convertimos esa porción a GPU (si xp=cp) o la dejamos como está (si xp=np)\n",
    "            if xp is cp:\n",
    "                Xb = cp.asarray(X_batch)\n",
    "            else:\n",
    "                Xb = X_batch\n",
    "\n",
    "            # Forward en ese batch\n",
    "            y_pred_batch = self.forward(Xb)  # (batch_size_eval, 10)\n",
    "\n",
    "            # Extraemos las etiquetas predichas (argmax) y pasamos a NumPy\n",
    "            if xp is cp:\n",
    "                y_pred_cpu = cp.asnumpy(xp.argmax(y_pred_batch, axis=1))\n",
    "            else:\n",
    "                y_pred_cpu = np.argmax(y_pred_batch, axis=1)\n",
    "\n",
    "            all_preds.append(y_pred_cpu)\n",
    "\n",
    "            # Opcionalmente: liberar memoria pool de CuPy\n",
    "            if xp is cp:\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "        # Concatenamos todas las predicciones\n",
    "        y_pred_labels = np.concatenate(all_preds, axis=0)  # forma (N,)\n",
    "\n",
    "        # Ahora podemos calcular métricas en CPU\n",
    "        print(\"\\nExactitud:\", accuracy_score(y, y_pred_labels))\n",
    "        print(\"\\nReporte de Clasificación:\")\n",
    "        print(classification_report(y, y_pred_labels))\n",
    "\n",
    "        cm = confusion_matrix(y, y_pred_labels)\n",
    "        plot_combined_confusion_matrix(cm, classes=[str(i) for i in range(10)])\n",
    "\n",
    "    def visualize_filters(self, layer_name=\"conv1\", figsize=(15, 10)):\n",
    "        \"\"\"\n",
    "        Visualiza los filtros aprendidos de una capa convolucional,\n",
    "        mostrando también el valor numérico de cada celda.\n",
    "        Args:\n",
    "            layer_name: \"conv1\" o \"conv2\"\n",
    "            figsize: tamaño de la figura\n",
    "        \"\"\"\n",
    "        if layer_name == \"conv1\":\n",
    "            # Obtiene los filtros de la primera capa (KH, KW, 1, out_c)\n",
    "            filters = self.conv1.get_filters()\n",
    "            layer_title = \"Primera Capa Convolucional\"\n",
    "        elif layer_name == \"conv2\":\n",
    "            # Obtiene los filtros de la segunda capa (KH, KW, in_c, out_c)\n",
    "            filters = self.conv2.get_filters()\n",
    "            layer_title = \"Segunda Capa Convolucional\"\n",
    "        else:\n",
    "            print(\"Capa no encontrada. Use 'conv1' o 'conv2'\")\n",
    "            return\n",
    "\n",
    "        # filters.shape = (k_h, k_w, in_c, out_c)\n",
    "        k_h, k_w, in_c, out_c = filters.shape\n",
    "\n",
    "        # Determinar cuántos filtros mostramos en un grid de hasta 8 columnas\n",
    "        n_cols = min(8, out_c)\n",
    "        n_rows = (out_c + n_cols - 1) // n_cols\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.suptitle(\n",
    "            f'{layer_title} - Filtros Aprendidos\\n'\n",
    "            f'Cada imagen muestra qué patrón detecta cada filtro, con sus valores',\n",
    "            fontsize=14, y=0.95\n",
    "        )\n",
    "\n",
    "        # Recorremos cada filtro (f = índice del filtro de salida)\n",
    "        for f in range(min(out_c, 32)):  # Mostramos como máximo 32 filtros\n",
    "            plt.subplot(n_rows, n_cols, f + 1)\n",
    "\n",
    "            # Si in_c == 1 (primera capa), tomamos el único canal,\n",
    "            # si in_c > 1 (segunda capa), promediamos sobre todos los canales de entrada.\n",
    "            if in_c == 1:\n",
    "                # primer canal (solo uno)\n",
    "                filter_weights = filters[:, :, 0, f]  # shape: (k_h, k_w)\n",
    "            else:\n",
    "                # promedio sobre los in_c canales\n",
    "                filter_weights = np.mean(filters[:, :, :, f], axis=2)  # (k_h, k_w)\n",
    "\n",
    "            # Normalizar para imshow (entre 0 y 1) solo para la paleta de colores:\n",
    "            mn, mx = filter_weights.min(), filter_weights.max()\n",
    "            if abs(mx - mn) < 1e-8:\n",
    "                filter_img = np.zeros_like(filter_weights)\n",
    "            else:\n",
    "                filter_img = (filter_weights - mn) / (mx - mn)\n",
    "\n",
    "            plt.imshow(filter_img, cmap='viridis', interpolation='nearest')\n",
    "            plt.title(f'Filtro {f+1}', fontsize=10)\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Imprimir el valor numérico de cada celda sobre la imagen\n",
    "            for row in range(k_h):\n",
    "                for col in range(k_w):\n",
    "                    v = filter_weights[row, col]\n",
    "                    bg = filter_img[row, col]\n",
    "                    color = 'white' if bg < 0.5 else 'black'\n",
    "                    plt.text(\n",
    "                        col, row,             # (x=col, y=row) en coordenadas del subplot\n",
    "                        f'{v:.2f}',           # texto con dos decimales\n",
    "                        ha='center', va='center',\n",
    "                        color=color, fontsize=8\n",
    "                    )\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "        plt.show()\n",
    "\n",
    "        # Explicación breve\n",
    "        print(f\"\\nInterpretación de {layer_title}:\")\n",
    "        if layer_name == \"conv1\":\n",
    "            print(\"• Cada filtro (3×3) detecta patrones básicos: bordes horizontales, verticales o pequeñas esquinas.\")\n",
    "            print(\"• Los valores positivos (hacia amarillo) realzan las zonas donde ese patrón aparece en la imagen.\")\n",
    "            print(\"• Los valores negativos (hacia morado) atenúan zonas donde el patrón no encaja.\")\n",
    "        else:\n",
    "            print(\"• En la segunda capa, cada filtro combina la salida de Conv1 (32 canales) para formas más complejas.\")\n",
    "            print(\"• Al promediar sobre los canales de entrada ves el “peso global” que cada filtro otorga en posición (i,j).\")\n",
    "            print(\"• Filas con valores positivos fuertes indican que ese parche responde fuertemente a combinaciones de bordes obtenidos en Conv1.\")\n",
    "\n",
    "    def visualize_feature_maps(self, X_sample, y_sample=None, sample_idx=0, figsize=(20, 12)):\n",
    "        \"\"\"\n",
    "        Visualiza los mapas de características para una imagen específica.\n",
    "\n",
    "        Args:\n",
    "            X_sample: imagen(es) de entrada. Puede tener forma (H,W,1) o (N,H,W,1).\n",
    "            y_sample: etiqueta(s) real(es), opcional (para mostrar el dígito).\n",
    "            sample_idx: índice de la muestra a visualizar si hay varias.\n",
    "            figsize: tamaño de la figura.\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "\n",
    "        # 1) Asegurarnos de que X_sample tenga forma (N, H, W, 1)\n",
    "        if X_sample.ndim == 3:\n",
    "            X_sample = X_sample[np.newaxis, ...]  # -> (1, H, W, 1)\n",
    "\n",
    "        # 2) Convertir X_sample a arreglo del backend xp (CuPy o NumPy)\n",
    "        if xp is cp:\n",
    "            X_input = cp.asarray(X_sample)\n",
    "        else:\n",
    "            X_input = X_sample\n",
    "\n",
    "        # 3) Ejecutar forward sobre ese arreglo xp para llenar last_output en conv1 y conv2\n",
    "        _ = self.forward(X_input)\n",
    "\n",
    "        # 4) Extraer la imagen original (en NumPy) para mostrar\n",
    "        if xp is cp:\n",
    "            original_img = cp.asnumpy(X_input[sample_idx, :, :, 0])\n",
    "        else:\n",
    "            original_img = X_input[sample_idx, :, :, 0]\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "        # --- Mostrar imagen original ---\n",
    "        plt.subplot(3, 8, 1)\n",
    "        plt.imshow(original_img, cmap='gray')\n",
    "        title = \"Imagen Original\"\n",
    "        if y_sample is not None:\n",
    "            # Convertir etiqueta a NumPy si fuera CuPy\n",
    "            if xp is cp:\n",
    "                y_cpu = cp.asnumpy(y_sample)\n",
    "            else:\n",
    "                y_cpu = y_sample\n",
    "\n",
    "            if y_cpu.ndim > 0:\n",
    "                label = y_cpu[sample_idx]\n",
    "            else:\n",
    "                label = y_cpu\n",
    "            title += f\"\\nDígito: {label}\"\n",
    "        plt.title(title, fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # --- Mapas de características de la primera convolución ---\n",
    "        # Conv1 almacena su salida en self.conv1.last_output\n",
    "        fmap1 = self.conv1.get_feature_maps(sample_idx)  # forma: (OH1, OW1, 32)\n",
    "        OH1, OW1, C1 = fmap1.shape\n",
    "        for i in range(min(7, C1)):\n",
    "            plt.subplot(3, 8, i + 2)\n",
    "            plt.imshow(fmap1[:, :, i], cmap='viridis')\n",
    "            plt.title(f'Conv1 - Mapa {i+1}', fontsize=10)\n",
    "            plt.axis('off')\n",
    "\n",
    "        # --- Mapas después del primer pooling ---\n",
    "        pool1_out = self.pool1.last_output  # xp-array de forma (N, OH1/2, OW1/2, 32)\n",
    "        if xp is cp:\n",
    "            pool1_out_cpu = cp.asnumpy(pool1_out[sample_idx])\n",
    "        else:\n",
    "            pool1_out_cpu = pool1_out[sample_idx]\n",
    "        _, _, C2 = pool1_out_cpu.shape\n",
    "        for i in range(min(8, C2)):\n",
    "            plt.subplot(3, 8, i + 9)\n",
    "            plt.imshow(pool1_out_cpu[:, :, i], cmap='viridis')\n",
    "            plt.title(f'Pool1 - Mapa {i+1}', fontsize=10)\n",
    "            plt.axis('off')\n",
    "\n",
    "        # --- Mapas de la segunda convolución ---\n",
    "        fmap2 = self.conv2.get_feature_maps(sample_idx)  # (OH2, OW2, 64)\n",
    "        _, _, C3 = fmap2.shape\n",
    "        for i in range(min(8, C3)):\n",
    "            plt.subplot(3, 8, i + 17)\n",
    "            plt.imshow(fmap2[:, :, i], cmap='plasma')\n",
    "            plt.title(f'Conv2 - Mapa {i+1}', fontsize=10)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle('Mapas de Características\\n'\n",
    "                     'Fila 1: Original + Conv1 | Fila 2: Pool1 | Fila 3: Conv2',\n",
    "                     fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Texto explicativo breve\n",
    "        print(\"\\nInterpretación de los Mapas de Características:\")\n",
    "        print(\"• CONV1: Detecta bordes y patrones básicos (líneas horizontales, verticales, etc.).\")\n",
    "        print(\"• POOL1: Reduce resolución manteniendo las activaciones más fuertes.\")\n",
    "        print(\"• CONV2: Combina características de CONV1 para formar patrones más complejos.\")\n",
    "\n",
    "    def analyze_layer_responses(self, X_samples, y_samples, num_samples=5):\n",
    "        \"\"\"\n",
    "        Analiza cómo responden las capas a diferentes dígitos (resumen estadístico).\n",
    "        \"\"\"\n",
    "        xp = self.xp\n",
    "        print(\"Analizando respuestas de las capas a diferentes dígitos...\\n\")\n",
    "\n",
    "        # Tomamos hasta num_samples ejemplos y hacemos forward\n",
    "        _ = self.forward(X_samples[:num_samples])\n",
    "\n",
    "        for name, layer in [(\"Conv1\", self.conv1), (\"Conv2\", self.conv2)]:\n",
    "            fmap = layer.get_feature_maps(0)  # mapa de la primera muestra\n",
    "            print(f\"{name}:\")\n",
    "            print(f\"   • Dimensiones de salida: {fmap.shape}\")\n",
    "            print(f\"   • Número de filtros: {fmap.shape[2]}\")\n",
    "            print(f\"   • Rango de activación: [{fmap.min():.3f}, {fmap.max():.3f}]\")\n",
    "            sparsity = np.mean(fmap > 0) * 100\n",
    "            print(f\"   • Escasez (% neuronas activas): {sparsity:.1f}%\\n\")\n",
    "\n",
    "# 5. Función main adaptada que incluye visualizaciones\n",
    "def main(device='gpu'):\n",
    "    \"\"\"\n",
    "    device: 'gpu' o 'cpu'.    \n",
    "    \"\"\"\n",
    "    xp = get_xp(device)\n",
    "    if device == 'gpu' and xp is np:\n",
    "        print(\"No se detectó CuPy/GPU; se usará CPU (NumPy).\")\n",
    "    elif xp is cp:\n",
    "        print(\"Usando GPU (CuPy) para operaciones numéricas.\")\n",
    "    else:\n",
    "        print(\"Usando CPU (NumPy) para operaciones numéricas.\")\n",
    "\n",
    "    # 1. Cargar datos (siempre en CPU, luego convertimos a xp en load_mnist_data)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_mnist_data(xp=xp)\n",
    "\n",
    "    # 2. Crear y entrenar modelo\n",
    "    print(\"\\nEntrenando CNN...\")\n",
    "    model = VisualizableCNN(xp=xp)\n",
    "    history = model.train(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=35,\n",
    "        batch_size=64,\n",
    "        lr=0.001\n",
    "    )\n",
    "\n",
    "    # 3. Mostrar métricas finales en test\n",
    "    print(\"\\nEvaluación en Test:\")\n",
    "    model.evaluate(to_cpu(X_test, xp), to_cpu(y_test, xp))\n",
    "\n",
    "    # 4. Visualización de filtros\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"1. VISUALIZACIÓN DE FILTROS APRENDIDOS\")\n",
    "    print(\"=\"*50)\n",
    "    model.visualize_filters(\"conv1\")\n",
    "    model.visualize_filters(\"conv2\")\n",
    "\n",
    "    # 5. Visualización de mapas de características en algunas muestras\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"2. VISUALIZACIÓN DE MAPAS DE CARACTERÍSTICAS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Seleccionamos las primeras 3 muestras de test (NumPy para visualización)\n",
    "    # Si xp es CuPy, convertimos a NumPy para mostrar\n",
    "    X_test_cpu = to_cpu(X_test, xp)\n",
    "    y_test_cpu = to_cpu(y_test, xp)\n",
    "    for i in range(min(3, X_test_cpu.shape[0])):\n",
    "        print(f\"\\n--- Muestra {i+1} ---\")\n",
    "        model.visualize_feature_maps(X_test_cpu[i], y_test_cpu, sample_idx=0)\n",
    "\n",
    "    # 6. Análisis resumido de respuestas de las capas\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"3. ANÁLISIS DE RESPUESTAS DE CAPAS\")\n",
    "    print(\"=\"*50)\n",
    "    # Tomamos 5 muestras aleatorias del test\n",
    "    idxs = np.random.choice(X_test_cpu.shape[0], size=5, replace=False)\n",
    "    X_samples = X_test_cpu[idxs]\n",
    "    y_samples = y_test_cpu[idxs]\n",
    "    # Convertimos a xp antes de analizar si fuera necesario\n",
    "    if xp is not np:\n",
    "        X_samples = xp.asarray(X_samples)\n",
    "        y_samples = xp.asarray(y_samples)\n",
    "    model.analyze_layer_responses(X_samples, y_samples)\n",
    "\n",
    "    # 7. Graficar curvas de entrenamiento (loss / acc)\n",
    "    loss_train = np.array(history['loss'])\n",
    "    loss_val = np.array(history['val_loss'])\n",
    "    acc_train = np.array(history['acc'])\n",
    "    acc_val = np.array(history['val_acc'])\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_train, label='Train')\n",
    "    plt.plot(loss_val, label='Validation')\n",
    "    plt.title('Pérdida por Época')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(acc_train, label='Train')\n",
    "    plt.plot(acc_val, label='Validation')\n",
    "    plt.title('Exactitud por Época')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Exactitud')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 8. Limpiar para liberar GPU\n",
    "    if xp is cp:\n",
    "        # 8.1. Eliminar referencias a objetos grandes\n",
    "        del model\n",
    "        del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "        del history, loss_train, loss_val, acc_train, acc_val\n",
    "\n",
    "        # 8.2. Forzar recolección de basura de Python\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        # 8.3. Liberar los memory pools de CuPy\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "\n",
    "        print(\"Memoria GPU liberada.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejecutar en GPU si está disponible\n",
    "    main(device='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528b858-60e5-4d9d-b4b8-af164523bc1b",
   "metadata": {},
   "source": [
    "# Ejercicio: CNN con Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f1a6a-1171-48d5-92e0-04e5aa4c2efd",
   "metadata": {},
   "source": [
    "Investigue como reproducir el ejemplo sin visualización de características anterior con Pytorch, en lugar de utilizar un enfoque \"a pie\". Deben haber similitudes con el código \"a pie\". Sin embargo, será un código mucho más suscinto. Por ejemplo, las clases como Conv2D, MaxPool2D, Flatten y Dense ya están programadas. Incluso no se necesita un método como *backward* porque ya está programado. Escriba el código con Pytorch a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed01adb-0af0-4541-909b-cd76e0c202cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dde22a12-ab9a-4100-b99e-9158ff07d22b",
   "metadata": {},
   "source": [
    "# Ejercicio: corrida en el cluster institucional\n",
    "\n",
    "**¡Cambie mi.usuario.institucional por su usuario en el archivo .sh!**\n",
    "\n",
    "La corrida en el cluster produce una salida `run_mnist_????.out`. Súbala a Mediación Virtual. Es parte de la tarea. Lea bien las instrucciones abajo.\n",
    "\n",
    "Para que esto funcione, además, debe crear un ambiente Python. Necesita `uv`. Use `curl` para descargar el instalador y ejecútelo con `sh`:\n",
    "\n",
    "```bash\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "\n",
    "Si su sistema no tiene `curl`, puede utilizar `wget`:\n",
    "\n",
    "```bash\n",
    "wget -qO- https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "\n",
    "Luego, una vez instalado `uv` instale Python 3.12 y Pytorch:\n",
    "\n",
    "```bash\n",
    "uv venv py3.12_pytorch2 --python 3.12\n",
    "source py3.12_pytorch2/bin/activate\n",
    "uv pip install torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "Instale también cualquier otra biblioteca necesaria como matplotlib utilizando `uv pip`. Recuerde siempre activar el ambiente antes de ejecutar comandos con `source py3.12_pytorch2/bin/activate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3d5c3-fb7c-492d-a90a-efd9105c89b3",
   "metadata": {},
   "source": [
    "## Instrucciones para correr el ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b11e1e-5e9e-42b4-b97a-711268c4f3ea",
   "metadata": {},
   "source": [
    "1. Cargue al cluster institucional el ejercicio Pytorch de red neuronal convolucional con nombre mnist.py.\n",
    "   \n",
    "2. Cree un archivo run_mnist.sh con el siguiente contenido:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=mnist\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --account=2025-I-ci0148\n",
    "#SBATCH --time=1:00:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --gpus-per-node=1\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --output=run_mnist_%j.out\n",
    "#SBATCH --error=run_mnist_%j.err\n",
    "#SBATCH --mail-user=mi.usuario.institucional@ucr.ac.cr\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "# Cargar CUDA\n",
    "module load cuda/12.5\n",
    "\n",
    "# Configurar CUDA y NVIDIA HPC SDK\n",
    "export NVHPC_ROOT=/opt/nvidia/hpc_sdk/Linux_x86_64/24.5\n",
    "export CUDA_HOME=$NVHPC_ROOT/cuda/12.4\n",
    "export PATH=$CUDA_HOME/bin:$PATH\n",
    "export LD_LIBRARY_PATH=$CUDA_HOME/targets/x86_64-linux/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Imprimir variables de entorno para diagnóstico\n",
    "echo \"=== Variables de entorno ===\"\n",
    "echo \"NVHPC_ROOT: $NVHPC_ROOT\"\n",
    "echo \"CUDA_HOME: $CUDA_HOME\"\n",
    "echo \"LD_LIBRARY_PATH: $LD_LIBRARY_PATH\"\n",
    "echo \"\"\n",
    "\n",
    "# Activar el entorno Python\n",
    "source /home/mi.usuario.institucional/py3.12_pytorch2/bin/activate\n",
    "\n",
    "# Verificar que CUDA está funcionando\n",
    "echo \"=== Verificando CUDA con Python ===\"\n",
    "python -c \"\n",
    "import sys\n",
    "import os\n",
    "print('Python version:', sys.version)\n",
    "print('CUDA_HOME:', os.environ.get('CUDA_HOME'))\n",
    "print('LD_LIBRARY_PATH:', os.environ.get('LD_LIBRARY_PATH'))\n",
    "\n",
    "import torch\n",
    "print('\\nPyTorch info:')\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('CUDA version:', torch.version.cuda)\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU count:', torch.cuda.device_count())\n",
    "    print('GPU name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print('\\nCuPy info:')\n",
    "    print('CuPy version:', cp.__version__)\n",
    "    print('CuPy CUDA version:', cp.cuda.runtime.runtimeGetVersion(), '\\n')\n",
    "except Exception as e:\n",
    "    print('\\nError importing CuPy:', str(e), '\\n')\n",
    "\"\n",
    "\n",
    "set -x\n",
    "# Ejecutar el script\n",
    "srun python ~/mnist.py\n",
    "```\n",
    "\n",
    "3. Puede ver su trabajo en cola con el siguiente comando:\n",
    "\n",
    "```bash\n",
    "watch 'squeue --format=\"%.6i %.9P %.10j %.15u %.2t %.9M %.6D %.4C %.10b %.17R\"'\n",
    "```\n",
    "\n",
    "4. Debería recibir un correo electrónico si falla o si termina el trabajo. Si falla hay un archivo con extensión .err que indica los errores. Cuando ya corra bien, hay otro archivo .out con la salidad de la corrida.  Suba a Mediación Virtual el archivo .out como prueba de que corrió bien el ejemplo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
